# Feature Breakdown #3: í”„ë¡œì íŠ¸ë³„ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ

**ì‘ì„±ì¼**: 2025-09-26
**ëŒ€ìƒ**: í”„ë¡œì íŠ¸ë³„ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ - ë¬´ì œí•œ ëŒ€í™” ì§€ì› ë° ìŠ¤ë§ˆíŠ¸ ì •ë¦¬

---

## ë¬¸ì œ ë¶„ì„

### 1. ë¬¸ì œ ì •ì˜ ë° ë³µì¡ì„± í‰ê°€
- **ë¬¸ì œ**: í”„ë¡œì íŠ¸ë³„ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ êµ¬í˜„ - ë¬´ì œí•œ ëŒ€í™” ì§€ì› ë° ìŠ¤ë§ˆíŠ¸ ì •ë¦¬
- **ë³µì¡ì„± ìˆ˜ì¤€**: ë†’ìŒ
- **ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 12ì¼ (2.5ì£¼)
- **ì£¼ìš” ë„ì „ ê³¼ì œ**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬, ì˜ë¯¸ ê²€ìƒ‰ êµ¬í˜„, ìë™ ì¤‘ìš”ë„ íŒì •, ë²¡í„° ì„ë² ë”© í†µí•©

### 2. ë²”ìœ„ ë° ì œì•½ì¡°ê±´
- **í¬í•¨ ë²”ìœ„**: SQLite ê¸°ë°˜ ë©”ëª¨ë¦¬ ì €ì¥, ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ, ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰, CLI/Desktop UI, ë²¡í„° ì„ë² ë”©
- **ì œì™¸ ë²”ìœ„**: í´ë¼ìš°ë“œ ë™ê¸°í™”, ë©€í‹°ìœ ì € ì§€ì›, ì‹¤ì‹œê°„ í˜‘ì—…
- **ì œì•½ì¡°ê±´**: ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ ë™ì‘, ê¸°ì¡´ AI CLIì™€ í˜¸í™˜ì„± ìœ ì§€, ì„±ëŠ¥ (1ì´ˆ ì´ë‚´ ê²€ìƒ‰)
- **ì „ì œì¡°ê±´**: í˜„ì¬ AI CLI ì‹œìŠ¤í…œì´ ì •ìƒ ë™ì‘, Qdrant ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš© ê°€ëŠ¥, ê¸°ë³¸ ì„ë² ë”© ê²½ë¡œëŠ” ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (í´ë¼ìš°ë“œ API ì‚¬ìš© ì‹œ ë³„ë„ opt-in ì„¤ì •)

---

## ì‘ì—… ë¶„í•´

### Phase 1: ê¸°ë³¸ ì €ì¥ ì‹œìŠ¤í…œ (3ì¼)
**ëª©í‘œ**: í”„ë¡œì íŠ¸ë³„ ëŒ€í™” ì €ì¥ ë° ê¸°ë³¸ ì¡°íšŒ ê¸°ëŠ¥ ì™„ì„±

| ì‘ì—… | ì„¤ëª… | ì™„ë£Œ ê¸°ì¤€ (DoD) | ìš°ì„ ìˆœìœ„ |
|------|------|-----------------|----------|
| ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„ | SQLite í…Œì´ë¸” êµ¬ì¡° ë° ì¸ë±ìŠ¤ ìµœì í™” | schema.sql íŒŒì¼ ì™„ì„± ë° í…ŒìŠ¤íŠ¸ í†µê³¼ | ë†’ìŒ |
| í”„ë¡œì íŠ¸ ì‹ë³„ ì‹œìŠ¤í…œ | ë””ë ‰í† ë¦¬ ê¸°ë°˜ í”„ë¡œì íŠ¸ í•´ì‹œ ìƒì„± | ë™ì¼ í”„ë¡œì íŠ¸ì—ì„œ ì¼ê´€ëœ í•´ì‹œ ìƒì„± | ë†’ìŒ |
| ê¸°ë³¸ ëŒ€í™” ì €ì¥ ë¡œì§ | conversations í…Œì´ë¸”ì— ëŒ€í™” ì €ì¥ | ëŒ€í™” ì €ì¥/ì¡°íšŒ ë‹¨ìœ„í…ŒìŠ¤íŠ¸ í†µê³¼ | ë†’ìŒ |
| JSON ë°±ì—… ì‹œìŠ¤í…œ | ì‹¤ì‹œê°„ JSON íŒŒì¼ ë°±ì—… | ë°±ì—… íŒŒì¼ ìƒì„± ë° ë³µì› ê¸°ëŠ¥ ë™ì‘ | ì¤‘ê°„ |

### Phase 2: ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ (2ì¼)
**ëª©í‘œ**: ì¤‘ìš”ë„ ê¸°ë°˜ ìë™ ì‚­ì œ ë° ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„

| ì‘ì—… | ì„¤ëª… | ì™„ë£Œ ê¸°ì¤€ (DoD) | ì˜ì¡´ì„± |
|------|------|-----------------|--------|
| ì¤‘ìš”ë„ ìë™ íŒì • ë¡œì§ | í‚¤ì›Œë“œ/ê¸¸ì´/ì½”ë“œ í¬í•¨ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° | íŒì • ì •í™•ë„ 85% ì´ìƒ | Phase 1 ì™„ë£Œ |
| TTL ê¸°ë°˜ ìë™ ì‚­ì œ | importance_scoreë³„ TTL ì ìš© | ë§Œë£Œëœ ëŒ€í™” ìë™ ì‚­ì œ í™•ì¸ | ì¤‘ìš”ë„ íŒì • ì™„ë£Œ |
| ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ | cron/scheduler ê¸°ë°˜ ì •ë¦¬ ì‘ì—… | ì¼ì • ê°„ê²©ìœ¼ë¡œ ìë™ ì •ë¦¬ ì‹¤í–‰ | TTL ì‚­ì œ ì™„ë£Œ |
| ì•ˆì „í•œ ì‚­ì œ í™•ì¸ ì‹œìŠ¤í…œ | ì‚­ì œ ì „ ì‚¬ìš©ì í™•ì¸ ë° ë¡¤ë°± | ì‹¤ìˆ˜ ì‚­ì œ ë°©ì§€ ê¸°ëŠ¥ ë™ì‘ | ìë™ ì‚­ì œ ì™„ë£Œ |

### Phase 3: ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ (3ì¼)
**ëª©í‘œ**: í‚¤ì›Œë“œ/ì˜ë¯¸ ê²€ìƒ‰ ë° ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìë™ í¬í•¨

| ì‘ì—… | ì„¤ëª… | ì™„ë£Œ ê¸°ì¤€ (DoD) | ìœ„í—˜ë„ |
|------|------|-----------------|--------|
| í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ | SQL LIKE ë° FTS ê²€ìƒ‰ êµ¬í˜„ | í‚¤ì›Œë“œ ê²€ìƒ‰ 1ì´ˆ ì´ë‚´ ì‘ë‹µ | ë‚®ìŒ |
| ì„ë² ë”© ìƒì„± ë° ì €ì¥ | ëŒ€í™”ë³„ ë²¡í„° ì„ë² ë”© ìƒì„± | conversation_embeddings í…Œì´ë¸” í™œìš© | ì¤‘ê°„ |
| Qdrant ë²¡í„° ì €ì¥ì†Œ ì—°ë™ | ëŒ€ìš©ëŸ‰ ì˜ë¯¸ ê²€ìƒ‰ìš© ì™¸ë¶€ ì €ì¥ì†Œ | ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì •ìƒ ë™ì‘ | ë†’ìŒ |
| ë²¡í„° ë™ê¸°í™” í/ì›Œì»¤ | Qdrant ì¥ì•  ëŒ€ë¹„ ì¬ì‹œë„ íŒŒì´í”„ë¼ì¸ | vector_sync_queue ë“œë ˆì¸ ì›Œì»¤ í…ŒìŠ¤íŠ¸ | ë†’ìŒ |
| ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° | í‚¤ì›Œë“œ+ì˜ë¯¸+ì‹œê°„+ì¤‘ìš”ë„ ì¢…í•© ì ìˆ˜ | ê´€ë ¨ì„± ì •í™•ë„ 80% ì´ìƒ | ì¤‘ê°„ |
| ì»¨í…ìŠ¤íŠ¸ ìë™ í¬í•¨ ë¡œì§ | ì§ˆë¬¸ ì‹œ ê´€ë ¨ ì´ì „ ëŒ€í™” ìë™ ì œê³µ | AI ì‘ë‹µ í’ˆì§ˆ ê°œì„  í™•ì¸ | ì¤‘ê°„ |

### Phase 4: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (2ì¼)
**ëª©í‘œ**: CLI ë° Desktop App UI êµ¬í˜„

| ì‘ì—… | ì„¤ëª… | ì™„ë£Œ ê¸°ì¤€ (DoD) | ìœ„í—˜ë„ |
|------|------|-----------------|--------|
| AI CLI ë©”ëª¨ë¦¬ ëª…ë ¹ì–´ | --memory ê´€ë ¨ 15ê°œ ëª…ë ¹ì–´ êµ¬í˜„ | ëª¨ë“  ëª…ë ¹ì–´ ì •ìƒ ë™ì‘ | ë‚®ìŒ |
| Desktop App ë©”ëª¨ë¦¬ UI | ë©”ëª¨ë¦¬ ê´€ë¦¬ íƒ­ ë° ê²€ìƒ‰ ê¸°ëŠ¥ | UI ì™„ì„± ë° ì‚¬ìš©ì„± í…ŒìŠ¤íŠ¸ í†µê³¼ | ì¤‘ê°„ |
| í†µê³„ ë° ì‹œê°í™” | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì¤‘ìš”ë„ ë¶„í¬ ì°¨íŠ¸ | í†µê³„ ì •ë³´ ì •í™•ì„± í™•ì¸ | ë‚®ìŒ |
| ì„¤ì • ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤ | ë³´ê´€ê¸°ê°„, ìë™ì •ë¦¬ ë“± ì„¤ì • UI | ì„¤ì • ë³€ê²½ ì¦‰ì‹œ ë°˜ì˜ | ë‚®ìŒ |

### Phase 5: ê³ ê¸‰ ê¸°ëŠ¥ (2ì¼)
**ëª©í‘œ**: AI ìš”ì•½, ì¤‘ìš” ì‚¬ì‹¤ ì¶”ì¶œ, ë°±ì—…/ë³µì›

| ì‘ì—… | ì„¤ëª… | ì™„ë£Œ ê¸°ì¤€ (DoD) | ìœ„í—˜ë„ |
|------|------|-----------------|--------|
| AI ìš”ì•½ ìƒì„± | ëŒ€í™” ê·¸ë£¹ë³„ AI ìš”ì•½ ìë™ ìƒì„± | ìš”ì•½ í’ˆì§ˆ ë§Œì¡±ë„ 80% ì´ìƒ | ì¤‘ê°„ |
| ì¤‘ìš” ì‚¬ì‹¤ ìë™ ì¶”ì¶œ | ì½”ë“œ/ì„¤ì •/ê²°ì •ì‚¬í•­ ìë™ ë¶„ë¥˜ | important_facts í…Œì´ë¸” í™œìš© | ë†’ìŒ |
| ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ | ì „ì²´ ë©”ëª¨ë¦¬ ë°±ì—… ë° ë³µì› ê¸°ëŠ¥ | ì™„ì „ ë°±ì—…/ë³µì› ì„±ê³µ | ë‚®ìŒ |
| ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ | ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ | ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ëª¨ë‹ˆí„°ë§ | ë‚®ìŒ |

### ì‚°ì¶œë¬¼
```mermaid
graph TD
    A[Phase 1: ê¸°ë³¸ ì €ì¥] --> B[Phase 2: ìë™ ì •ë¦¬]
    B --> C[Phase 3: ê²€ìƒ‰ ì‹œìŠ¤í…œ]
    C --> D[Phase 4: UI êµ¬í˜„]
    D --> E[Phase 5: ê³ ê¸‰ ê¸°ëŠ¥]

    A1[DB ìŠ¤í‚¤ë§ˆ] --> A2[í”„ë¡œì íŠ¸ ì‹ë³„]
    A2 --> A3[ëŒ€í™” ì €ì¥]
    A3 --> A4[JSON ë°±ì—…]

    B1[ì¤‘ìš”ë„ íŒì •] --> B2[TTL ì‚­ì œ]
    B2 --> B3[ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬]

    C1[í‚¤ì›Œë“œ ê²€ìƒ‰] --> C2[ì„ë² ë”© ìƒì„±]
    C2 --> C3[Qdrant ì—°ë™]
    C3 --> C4[ê´€ë ¨ì„± ê³„ì‚°]
    C4 --> C5[ì»¨í…ìŠ¤íŠ¸ í¬í•¨]
```

---

## ì‹¤í–‰ ê³„íš

### ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤
```
ê¸´ê¸‰ & ì¤‘ìš”           | ì¤‘ìš”í•˜ì§€ë§Œ ëœ ê¸´ê¸‰
- DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„      | - AI ìš”ì•½ ìƒì„±
- ê¸°ë³¸ ì €ì¥ ë¡œì§      | - Desktop App UI
- í”„ë¡œì íŠ¸ ì‹ë³„       | - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ê¸´ê¸‰í•˜ì§€ë§Œ ëœ ì¤‘ìš”    | ëœ ì¤‘ìš” & ëœ ê¸´ê¸‰
- CLI ëª…ë ¹ì–´ êµ¬í˜„     | - ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ
- í‚¤ì›Œë“œ ê²€ìƒ‰         | - ì¤‘ìš” ì‚¬ì‹¤ ì¶”ì¶œ
```

### ë§ˆì¼ìŠ¤í†¤
- **Week 1 (Day 1-5)**: Phase 1 + Phase 2 ì™„ë£Œ (ê¸°ë³¸ ì €ì¥ + ìë™ ì •ë¦¬)
- **Week 2 (Day 6-10)**: Phase 3 + Phase 4 ì™„ë£Œ (ê²€ìƒ‰ ì‹œìŠ¤í…œ + UI)
- **Week 3 (Day 11-12)**: Phase 5 ì™„ë£Œ (ê³ ê¸‰ ê¸°ëŠ¥)

### ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ ë°©ì•ˆ
| ìœ„í—˜ ìš”ì†Œ | ê°€ëŠ¥ì„± | ì˜í–¥ë„ | ëŒ€ì‘ ë°©ì•ˆ |
|-----------|--------|--------|-----------|
| Qdrant ì—°ë™ ë³µì¡ì„± | ë†’ìŒ | ì¤‘ê°„ | ë¡œì»¬ SQLite ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥ |
| ì„ë² ë”© ì„±ëŠ¥ ë¬¸ì œ | ì¤‘ê°„ | ë†’ìŒ | ë°°ì¹˜ ì²˜ë¦¬ + ìºì‹±ìœ¼ë¡œ ìµœì í™” |
| ì¤‘ìš”ë„ íŒì • ì •í™•ë„ | ë†’ìŒ | ì¤‘ê°„ | ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì§€ì† ê°œì„  |
| UI ë³µì¡ì„± | ì¤‘ê°„ | ë‚®ìŒ | CLI ìš°ì„ , Desktop UIëŠ” ë‹¨ìˆœí™” |

---

## ì„¸ë¶€ ì‹¤í–‰ ì‘ì—… ë¦¬ìŠ¤íŠ¸

### ğŸ“ **Phase 1: ê¸°ë³¸ ì €ì¥ ì‹œìŠ¤í…œ (3ì¼)**

**1.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ êµ¬í˜„**
```sql
-- schema.sql ìƒì„±
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    user_query TEXT NOT NULL,
    ai_response TEXT NOT NULL,
    model_used VARCHAR(50),
    importance_score INTEGER DEFAULT 5,
    session_id VARCHAR(50),
    token_count INTEGER,
    response_time_ms INTEGER,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ëŒ€í™” íƒœê·¸ (ì •ê·œí™”)
CREATE TABLE conversation_tags (
    conversation_id INTEGER NOT NULL,
    tag TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (conversation_id, tag),
    FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- ëŒ€í™” ìš”ì•½
CREATE TABLE conversation_summaries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date_range TEXT,              -- "2024-09-01 to 2024-09-07"
    summary TEXT,                 -- AI ìƒì„± ìš”ì•½
    conversation_count INTEGER,
    importance_level INTEGER,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ì¤‘ìš” ì‚¬ì‹¤
CREATE TABLE important_facts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    fact TEXT NOT NULL,
    category VARCHAR(100),        -- code, config, decision, etc.
    source_conversation_id INTEGER,
    user_marked BOOLEAN DEFAULT FALSE,
    ai_suggested BOOLEAN DEFAULT FALSE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_conversation_id) REFERENCES conversations(id)
);

-- ì˜ë¯¸ ê²€ìƒ‰ìš© ì„ë² ë”©
CREATE TABLE conversation_embeddings (
    conversation_id INTEGER PRIMARY KEY,
    embedding BLOB NOT NULL,                     -- 1536 float32 ë²¡í„° ì§ë ¬í™”
    vector_store_id TEXT,                        -- Qdrant ë“± ì™¸ë¶€ ìŠ¤í† ì–´ ì‹ë³„ì
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- ë²¡í„° ë™ê¸°í™” í (Qdrant ì¥ì•  ëŒ€ë¹„ ì¬ì‹œë„)
CREATE TABLE vector_sync_queue (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    conversation_id INTEGER NOT NULL,
    operation TEXT NOT NULL CHECK(operation IN ('upsert', 'delete')),
    payload TEXT,
    retries INTEGER DEFAULT 0,
    last_error TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_conversations_timestamp ON conversations(timestamp);
CREATE INDEX idx_conversations_importance ON conversations(importance_score);
CREATE INDEX idx_conversation_tags_tag ON conversation_tags(tag);
CREATE INDEX idx_vector_sync_queue_status ON vector_sync_queue(operation, retries);
CREATE UNIQUE INDEX idx_vector_sync_queue_unique ON vector_sync_queue(conversation_id, operation);
```

> ëª¨ë“  SQLite ì»¤ë„¥ì…˜ì€ `PRAGMA foreign_keys = ON` ë° JSON1 í™•ì¥ì´ í™œì„±í™”ë˜ì–´ ìˆë‹¤ëŠ” ì „ì œ í•˜ì— ë™ì‘.

**1.2 í”„ë¡œì íŠ¸ ì‹ë³„ ì‹œìŠ¤í…œ**
```python
import hashlib
import os
from pathlib import Path

def get_project_hash(project_path: str) -> str:
    """í”„ë¡œì íŠ¸ ê²½ë¡œ ê¸°ë°˜ í•´ì‹œ ìƒì„±"""
    return hashlib.sha256(os.path.abspath(project_path).encode()).hexdigest()[:16]

def get_project_memory_path(project_path: str) -> Path:
    """í”„ë¡œì íŠ¸ë³„ ë©”ëª¨ë¦¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ"""
    memory_root = os.environ.get('MEMORY_ROOT', '~/.local/share/local-ai/memory')
    return Path(memory_root).expanduser() / 'projects' / get_project_hash(project_path)
```

**1.3 ëŒ€í™” ì €ì¥ ë¡œì§**
```python
import sqlite3
from datetime import datetime
from typing import Dict, List, Optional

class MemoryManager:
    def __init__(self, project_path: str):
        self.project_path = project_path
        self.memory_path = get_project_memory_path(project_path)
        self.db_path = self.memory_path / 'memory.db'
        self._ensure_database()

    def _ensure_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±"""
        self.memory_path.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("PRAGMA foreign_keys = ON;")
            schema_path = Path(__file__).resolve().parent / "schema.sql"
            with open(schema_path, "r", encoding="utf-8") as fp:
                conn.executescript(fp.read())

    def _get_connection(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("PRAGMA foreign_keys = ON;")
        return conn

    def save_conversation(self, query: str, response: str, model: str, **kwargs):
        """ëŒ€í™”ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        importance_score = self.calculate_importance(query, response)
        session_id = kwargs.get('session_id', 'default')
        token_count = kwargs.get('token_count', 0)
        response_time = kwargs.get('response_time_ms', 0)

        with self._get_connection() as conn:
            conn.execute("""
                INSERT INTO conversations
                (user_query, ai_response, model_used, importance_score,
                 session_id, token_count, response_time_ms)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (query, response, model, importance_score,
                  session_id, token_count, response_time))

        # JSON ë°±ì—…ë„ ë™ì‹œì— ìˆ˜í–‰
        self._backup_to_json()

    def get_conversations(self, limit: int = 50, min_importance: int = 1):
        """ì €ì¥ëœ ëŒ€í™” ì¡°íšŒ"""
        with self._get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM conversations
                WHERE importance_score >= ?
                ORDER BY timestamp DESC
                LIMIT ?
            """, (min_importance, limit))
            return [dict(row) for row in cursor.fetchall()]
```

### ğŸ§¹ **Phase 2: ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ (2ì¼)**

**2.1 ì¤‘ìš”ë„ ìë™ íŒì •**
```python
def calculate_importance_score(query: str, response: str, context: dict = None) -> int:
    """ìë™ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (1-10)"""
    score = 5  # ê¸°ë³¸ê°’

    # í‚¤ì›Œë“œ ë¶„ì„
    high_importance_keywords = [
        "ì„¤ì •", "config", "í™˜ê²½ë³€ìˆ˜", "architecture", "design pattern",
        "ë²„ê·¸", "í•´ê²°", "ë¬¸ì œ", "ì˜¤ë¥˜", "ì—ëŸ¬", "fix", "bug",
        "êµ¬í˜„", "ì•Œê³ ë¦¬ì¦˜", "ìµœì í™”", "ì„±ëŠ¥", "performance"
    ]

    low_importance_keywords = [
        "ì•ˆë…•", "hello", "í…ŒìŠ¤íŠ¸", "test", "í™•ì¸", "ì²´í¬"
    ]

    query_lower = query.lower()
    response_lower = response.lower()

    def adjust(delta: int) -> None:
        nonlocal score
        score = max(1, min(10, score + delta))

    if any(word in query_lower or word in response_lower for word in high_importance_keywords):
        adjust(2)
    if any(word in query_lower or word in response_lower for word in low_importance_keywords):
        adjust(-2)

    # ì‘ë‹µ ê¸¸ì´ ê³ ë ¤ (ê¸´ ì‘ë‹µ = ë” ì¤‘ìš”)
    if len(response) > 1000:
        adjust(1)
    if len(response) > 2000:
        adjust(1)

    # ì½”ë“œ í¬í•¨ ì—¬ë¶€
    if "```" in response or "def " in response or "class " in response:
        adjust(1)

    # ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜ (contextì—ì„œ)
    if context:
        if context.get("user_saved", False):
            score = 10  # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
        if context.get("user_dismissed", False):
            score = min(score, 3)  # ì‚¬ìš©ìê°€ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  í‘œì‹œ

    return score

# ì¤‘ìš”ë„ë³„ TTL ì„¤ì •
IMPORTANCE_LEVELS = {
    1: {"name": "ì¦‰ì‹œì‚­ì œ", "ttl_days": 0, "description": "ì¸ì‚¬, í…ŒìŠ¤íŠ¸"},
    2: {"name": "ë‹¨ê¸°ë³´ê´€", "ttl_days": 3, "description": "ê°„ë‹¨í•œ ì§ˆë¬¸"},
    3: {"name": "1ì£¼ë³´ê´€", "ttl_days": 7, "description": "ì¼ë°˜ ëŒ€í™”"},
    4: {"name": "2ì£¼ë³´ê´€", "ttl_days": 14, "description": "ì •ë³´ì„± ì§ˆë¬¸"},
    5: {"name": "ê¸°ë³¸ë³´ê´€", "ttl_days": 30, "description": "ê¸°ë³¸ê°’"},
    6: {"name": "1ê°œì›”", "ttl_days": 30, "description": "ì½”ë“œ ê´€ë ¨"},
    7: {"name": "3ê°œì›”", "ttl_days": 90, "description": "í”„ë¡œì íŠ¸ ì„¤ì •"},
    8: {"name": "6ê°œì›”", "ttl_days": 180, "description": "ì¤‘ìš” ê²°ì •ì‚¬í•­"},
    9: {"name": "1ë…„ë³´ê´€", "ttl_days": 365, "description": "í•µì‹¬ ë¬¸ì„œí™”"},
    10: {"name": "ì˜êµ¬ë³´ê´€", "ttl_days": -1, "description": "ì‚¬ìš©ì ì¤‘ìš”í‘œì‹œ"}
}
```

**2.2 TTL ê¸°ë°˜ ìë™ ì‚­ì œ**
```python
from datetime import datetime, timedelta

def cleanup_expired_conversations(project_path: str, dry_run: bool = True):
    """ë§Œë£Œëœ ëŒ€í™” ì •ë¦¬"""
    manager = MemoryManager(project_path)

    deleted_count = 0
    for importance_level, config in IMPORTANCE_LEVELS.items():
        if config["ttl_days"] <= 0:  # ì¦‰ì‹œì‚­ì œ ë˜ëŠ” ì˜êµ¬ë³´ê´€
            if importance_level == 1:  # ì¦‰ì‹œì‚­ì œ
                if not dry_run:
                    with manager._get_connection() as conn:
                        stale_ids = [row[0] for row in conn.execute(
                            "SELECT id FROM conversations WHERE importance_score = 1"
                        ).fetchall()]

                        if stale_ids:
                            conn.execute(
                                "DELETE FROM conversations WHERE importance_score = 1"
                            )
                            conn.executemany(
                                "INSERT OR IGNORE INTO vector_sync_queue (conversation_id, operation, payload) VALUES (?, 'delete', json_object('queued_at', CURRENT_TIMESTAMP))",
                                [(conv_id,) for conv_id in stale_ids]
                            )
                            deleted_count += len(stale_ids)
            continue

        cutoff_date = datetime.now() - timedelta(days=config["ttl_days"])

        if not dry_run:
            with manager._get_connection() as conn:
                stale_ids = [row[0] for row in conn.execute("""
                    SELECT id FROM conversations
                    WHERE importance_score = ? AND created_at < ?
                """, (importance_level, cutoff_date)).fetchall()]

                if stale_ids:
                    conn.execute("""
                        DELETE FROM conversations
                        WHERE importance_score = ? AND created_at < ?
                    """, (importance_level, cutoff_date))
                    conn.executemany(
                        "INSERT OR IGNORE INTO vector_sync_queue (conversation_id, operation, payload) VALUES (?, 'delete', json_object('queued_at', CURRENT_TIMESTAMP))",
                        [(conv_id,) for conv_id in stale_ids]
                    )
                    deleted_count += len(stale_ids)
        else:
            # dry_run ëª¨ë“œ: ì‚­ì œ ì˜ˆì • ê°œìˆ˜ë§Œ í™•ì¸
            with manager._get_connection() as conn:
                cursor = conn.execute("""
                    SELECT COUNT(*) FROM conversations
                    WHERE importance_score = ? AND created_at < ?
                """, (importance_level, cutoff_date))
                count = cursor.fetchone()[0]
                print(f"ì¤‘ìš”ë„ {importance_level}: {count}ê°œ ëŒ€í™” ì‚­ì œ ì˜ˆì •")

    return deleted_count
```

### ğŸ” **Phase 3: ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ (3ì¼)**

**3.1 í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰**
```python
def search_conversations(query: str, project_path: str, limit: int = 10) -> List[Dict]:
    """í‚¤ì›Œë“œë¡œ ëŒ€í™” ê²€ìƒ‰"""
    manager = MemoryManager(project_path)

    # ê²€ìƒ‰ì–´ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
    keywords = query.lower().split()

    with manager._get_connection() as conn:
        conn.row_factory = sqlite3.Row

        # LIKE ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ ê²€ìƒ‰
        where_conditions = []
        params = []

        for keyword in keywords:
            where_conditions.append(
                "(LOWER(user_query) LIKE ? OR LOWER(ai_response) LIKE ?)"
            )
            params.extend([f"%{keyword}%", f"%{keyword}%"])

        sql = f"""
        SELECT *,
               (importance_score * 0.3 +
                (julianday('now') - julianday(timestamp)) * -0.1) as relevance_score
        FROM conversations
        WHERE {' AND '.join(where_conditions)}
        ORDER BY relevance_score DESC, importance_score DESC, timestamp DESC
        LIMIT ?
        """
        params.append(limit)

        cursor = conn.execute(sql, params)
        return [dict(row) for row in cursor.fetchall()]

# FTS (Full Text Search) í™œìš©í•˜ë ¤ë©´
def enable_fts_search(project_path: str):
    """ì „ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ í™œì„±í™”"""
    manager = MemoryManager(project_path)

    with manager._get_connection() as conn:
        # FTS ê°€ìƒ í…Œì´ë¸” ìƒì„±
        conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS conversations_fts
            USING fts5(user_query, ai_response, content='conversations', content_rowid='id')
        """)

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ FTS í…Œì´ë¸”ì— ë³µì‚¬
        conn.execute("""
            INSERT INTO conversations_fts(conversations_fts) VALUES('rebuild')
        """)
```

**3.2 ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰**
```python
import numpy as np
import json
from typing import List

def generate_embedding(text: str, use_openai: bool = False) -> List[float]:
    """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
    if use_openai:
        # OpenAI API ì‚¬ìš©
        import openai
        response = openai.Embedding.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response['data'][0]['embedding']
    else:
        # ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: sentence-transformers)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        embedding = model.encode(text)
        return embedding.tolist()

# NOTE: ê¸°ë³¸ê°’ì€ ì˜¤í”„ë¼ì¸ í˜¸í™˜ì„ ìœ„í•´ ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°,
#       í™˜ê²½ë³€ìˆ˜ `ENABLE_OPENAI_EMBEDDINGS=1` ë“±ì„ í†µí•´ opt-in ì‹œ `use_openai=True`ë¡œ ì „í™˜.

def store_conversation_embedding(conversation_id: int, text: str, project_path: str):
    """ëŒ€í™” ì„ë² ë”© ì €ì¥"""
    manager = MemoryManager(project_path)
    embedding = generate_embedding(text)

    # ë²¡í„°ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ì§ë ¬í™”
    embedding_blob = np.array(embedding, dtype=np.float32).tobytes()

    with manager._get_connection() as conn:
        current = conn.execute(
            "SELECT vector_store_id FROM conversation_embeddings WHERE conversation_id = ?",
            (conversation_id,)
        ).fetchone()
        persisted_vector_id = current[0] if current else None

        conn.execute("""
            INSERT INTO conversation_embeddings (conversation_id, embedding, vector_store_id)
            VALUES (?, ?, ?)
            ON CONFLICT(conversation_id) DO UPDATE SET
                embedding = excluded.embedding,
                vector_store_id = COALESCE(conversation_embeddings.vector_store_id, excluded.vector_store_id)
        """, (conversation_id, embedding_blob, persisted_vector_id))

        conn.execute("""
            INSERT OR IGNORE INTO vector_sync_queue (conversation_id, operation, payload)
            VALUES (?, 'upsert', json_object('queued_at', CURRENT_TIMESTAMP))
        """, (conversation_id,))

def find_similar_conversations(query_text: str, project_path: str, limit: int = 5) -> List[Dict]:
    """ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰"""
    manager = MemoryManager(project_path)
    query_embedding = generate_embedding(query_text)
    query_vector = np.array(query_embedding, dtype=np.float32)

    similarities = []

    with manager._get_connection() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute("""
            SELECT c.*, ce.embedding
            FROM conversations c
            JOIN conversation_embeddings ce ON c.id = ce.conversation_id
        """)

        for row in cursor.fetchall():
            stored_vector = np.frombuffer(row['embedding'], dtype=np.float32)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            cosine_sim = np.dot(query_vector, stored_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(stored_vector)
            )

            conv_dict = dict(row)
            conv_dict['similarity_score'] = float(cosine_sim)
            similarities.append(conv_dict)

    # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x['similarity_score'], reverse=True)
    return similarities[:limit]
```

**3.3 Qdrant ë²¡í„° ì €ì¥ì†Œ ì—°ë™**
```python
from qdrant_client import QdrantClient
from qdrant_client.http import models

def sync_embeddings_to_qdrant(
    project_path: str,
    qdrant_url: str = "http://localhost:6333",
    batch_size: int = 100,
) -> None:
    """SQLiteì˜ ì„ë² ë”©ì„ Qdrantì— ë™ê¸°í™”"""
    manager = MemoryManager(project_path)
    client = QdrantClient(url=qdrant_url)

    collection_name = f"memory_{get_project_hash(project_path)}"

    # ì»¬ë ‰ì…˜ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    try:
        client.get_collection(collection_name)
    except Exception:
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
            on_disk=True,
            timeout=30,
        )

    with manager._get_connection() as conn:
        conn.row_factory = sqlite3.Row
        queued = conn.execute("""
            SELECT id, conversation_id, operation
            FROM vector_sync_queue
            ORDER BY created_at
            LIMIT ?
        """, (batch_size,)).fetchall()

    if not queued:
        return

    upsert_ids = [row["conversation_id"] for row in queued if row["operation"] == "upsert"]
    delete_ids = [row["conversation_id"] for row in queued if row["operation"] == "delete"]

    try:
        points = []
        if upsert_ids:
            with manager._get_connection() as conn:
                conn.row_factory = sqlite3.Row
                placeholders = ",".join(["?"] * len(upsert_ids))
                rows = conn.execute(f"""
                    SELECT c.id, c.user_query, c.ai_response, c.importance_score, c.timestamp, ce.embedding
                    FROM conversations c
                    JOIN conversation_embeddings ce ON c.id = ce.conversation_id
                    WHERE c.id IN ({placeholders})
                """, upsert_ids).fetchall()

            for row in rows:
                embedding_vector = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                points.append(models.PointStruct(
                    id=row["id"],
                    vector=embedding_vector,
                    payload={
                        "project_path": project_path,
                        "importance_score": int(row["importance_score"]),
                        "timestamp": row["timestamp"],
                    }
                ))

        if points:
            client.upsert(collection_name=collection_name, points=points, wait=True)

        if delete_ids:
            client.delete(collection_name=collection_name, points_selector=models.PointIdsList(points=delete_ids))

    except Exception as exc:
        # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ì •ë³´ ì—…ë°ì´íŠ¸
        with manager._get_connection() as conn:
            conn.executemany(
                """
                UPDATE vector_sync_queue
                SET retries = retries + 1,
                    last_error = ?
                WHERE id = ?
                """,
                [(str(exc), row["id"]) for row in queued]
            )
        raise
    else:
        with manager._get_connection() as conn:
            if upsert_ids:
                conn.executemany(
                    """
                    UPDATE conversation_embeddings
                    SET vector_store_id = ?
                    WHERE conversation_id = ?
                    """,
                    [(f"{collection_name}:{conv_id}", conv_id) for conv_id in upsert_ids]
                )

            conn.executemany(
                "DELETE FROM vector_sync_queue WHERE id = ?",
                [(row["id"],) for row in queued]
            )


def drain_vector_sync_queue(project_path: str, interval_seconds: int = 60):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ íë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬"""
    import time
    while True:
        try:
            sync_embeddings_to_qdrant(project_path)
        except Exception as exc:
            # ì¬ì‹œë„ ê°„ê²©ì„ ëŠ˜ë¦¬ê³  ë¡œê¹…/ì•Œë¦¼ ì²˜ë¦¬
            print(f"[vector-sync] retry later: {exc}")
            time.sleep(interval_seconds * 2)
        else:
            time.sleep(interval_seconds)

def search_similar_in_qdrant(query_text: str, project_path: str, limit: int = 5):
    """Qdrantì—ì„œ ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰"""
    client = QdrantClient(url="http://localhost:6333")
    collection_name = f"memory_{get_project_hash(project_path)}"

    query_embedding = generate_embedding(query_text)
    try:
        search_result = client.search(
            collection_name=collection_name,
            query_vector=query_embedding,
            limit=limit,
            score_threshold=0.7
        )
    except Exception:
        # Qdrant ì¥ì•  ì‹œ ë¡œì»¬ SQLite ì„ë² ë”©ìœ¼ë¡œ í´ë°±
        return find_similar_conversations(query_text, project_path, limit)

    hits = []
    for hit in search_result:
        hits.append({
            "id": hit.id,
            "score": hit.score,
            "payload": hit.payload
        })

    # ì¤‘ìš”ë„ ë° ìµœì‹ ì„±ìœ¼ë¡œ 2ì°¨ ì •ë ¬
    def combined_score(item):
        payload = item["payload"]
        importance = payload.get("importance_score", 5)
        timestamp = payload.get("timestamp")

        recency_penalty = 0
        if timestamp:
            from datetime import datetime
            try:
                age_days = (datetime.utcnow() - datetime.fromisoformat(timestamp)).days
                recency_penalty = age_days * 0.02
            except ValueError:
                recency_penalty = 0

        return item["score"] + importance * 0.05 - recency_penalty

    hits.sort(key=combined_score, reverse=True)
    return hits[:limit]
```

### ğŸ–¥ï¸ **Phase 4: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (2ì¼)**

**4.1 AI CLI ë©”ëª¨ë¦¬ ëª…ë ¹ì–´**
```python
# scripts/ai.pyì— ì¶”ê°€í•  í•¨ìˆ˜ë“¤

def handle_memory_command(args):
    """ë©”ëª¨ë¦¬ ê´€ë ¨ ëª…ë ¹ì–´ ì²˜ë¦¬"""
    project_path = os.getcwd()
    manager = MemoryManager(project_path)

    if args.memory_action == "status":
        show_memory_status(manager)
    elif args.memory_action == "search":
        search_memory(manager, args.memory_query)
    elif args.memory_action == "save":
        save_last_conversation(manager)
    elif args.memory_action == "cleanup":
        cleanup_memory(manager, dry_run=args.dry_run)
    elif args.memory_action == "stats":
        show_memory_stats(manager)
    elif args.memory_action == "export":
        export_memory(manager, args.output_file)

def show_memory_status(manager):
    """ë©”ëª¨ë¦¬ ìƒíƒœ í‘œì‹œ"""
    stats = manager.get_stats()
    print(f"ğŸ“Š Memory Status for {manager.project_path}")
    print(f"Total conversations: {stats['total_count']}")
    print(f"Important (8-10): {stats['important_count']}")
    print(f"Recent (last 7 days): {stats['recent_count']}")
    print(f"Database size: {stats['db_size_mb']:.2f} MB")

def search_memory(manager, query):
    """ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
    results = manager.search_conversations(query, limit=10)
    print(f"ğŸ” Search results for '{query}':")

    for i, result in enumerate(results, 1):
        print(f"\n{i}. [{result['importance_score']}/10] {result['timestamp']}")
        print(f"Q: {result['user_query'][:100]}...")
        print(f"A: {result['ai_response'][:100]}...")

# CLI ì¸í„°í˜ì´ìŠ¤ í™•ì¥
def add_memory_arguments(parser):
    """ë©”ëª¨ë¦¬ ê´€ë ¨ ëª…ë ¹ì–´ ì¸ì ì¶”ê°€"""
    parser.add_argument('--memory', action='store_true', help='Memory management mode')
    parser.add_argument('--memory-action', choices=[
        'status', 'search', 'save', 'cleanup', 'stats', 'export', 'import'
    ], help='Memory action to perform')
    parser.add_argument('--memory-query', help='Search query for memory')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be done')
    parser.add_argument('--output-file', help='Output file for export')
```

**4.2 Desktop App UI ì»´í¬ë„ŒíŠ¸ (ê¸°ë³¸ êµ¬ì¡°)**
```javascript
// desktop-app/src/components/MemoryManager.js
import React, { useState, useEffect } from 'react';

const MemoryManager = () => {
    const [conversations, setConversations] = useState([]);
    const [searchQuery, setSearchQuery] = useState('');
    const [stats, setStats] = useState({});

    const searchMemory = async (query) => {
        try {
            const response = await fetch('/api/memory/search', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ query })
            });
            const results = await response.json();
            setConversations(results);
        } catch (error) {
            console.error('Memory search failed:', error);
        }
    };

    const updateImportance = async (conversationId, newScore) => {
        try {
            await fetch('/api/memory/update-importance', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    conversation_id: conversationId,
                    importance_score: newScore
                })
            });
            // UI ì—…ë°ì´íŠ¸
        } catch (error) {
            console.error('Importance update failed:', error);
        }
    };

    return (
        <div className="memory-manager">
            <div className="search-section">
                <input
                    type="text"
                    placeholder="Search conversations..."
                    value={searchQuery}
                    onChange={(e) => setSearchQuery(e.target.value)}
                    onKeyPress={(e) => e.key === 'Enter' && searchMemory(searchQuery)}
                />
                <button onClick={() => searchMemory(searchQuery)}>
                    Search
                </button>
            </div>

            <div className="stats-section">
                <div className="stat-card">
                    <h3>Total Conversations</h3>
                    <p>{stats.total_count}</p>
                </div>
                <div className="stat-card">
                    <h3>Important Items</h3>
                    <p>{stats.important_count}</p>
                </div>
            </div>

            <div className="conversations-list">
                {conversations.map(conv => (
                    <div key={conv.id} className="conversation-item">
                        <div className="importance-controls">
                            {[1,2,3,4,5,6,7,8,9,10].map(score => (
                                <button
                                    key={score}
                                    className={conv.importance_score === score ? 'active' : ''}
                                    onClick={() => updateImportance(conv.id, score)}
                                >
                                    {score}
                                </button>
                            ))}
                        </div>
                        <div className="conversation-content">
                            <p><strong>Q:</strong> {conv.user_query}</p>
                            <p><strong>A:</strong> {conv.ai_response.substring(0, 200)}...</p>
                            <small>{conv.timestamp} | Model: {conv.model_used}</small>
                        </div>
                    </div>
                ))}
            </div>
        </div>
    );
};

export default MemoryManager;
```

---

## í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê° ì‘ì—… ì™„ë£Œ ì‹œ í™•ì¸ì‚¬í•­
- [ ] ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€ í™•ì¸ (ë©”ëª¨ë¦¬ ì €ì¥/ê²€ìƒ‰/ì •ë¦¬)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼ (ê° ëª¨ë“ˆë³„)
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (1ì´ˆ ì´ë‚´ ê²€ìƒ‰ ì‘ë‹µ)
- [ ] ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ (ë°±ì—…/ë³µì› í¬í•¨)
- [ ] ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ í†µê³¼

### ì „ì²´ ì™„ë£Œ ê¸°ì¤€
- [ ] 100ë§Œê°œ ëŒ€í™” ì²˜ë¦¬ ê°€ëŠ¥ (ì„±ëŠ¥ ëª©í‘œ)
- [ ] 1ì´ˆ ì´ë‚´ ê²€ìƒ‰ ì‘ë‹µ (ì„±ëŠ¥ ëª©í‘œ)
- [ ] 99.9% ë°ì´í„° ì•ˆì •ì„± (ì•ˆì •ì„± ëª©í‘œ)
- [ ] ì œë¡œ ì„¤ì • ìë™ ë™ì‘ (ì‚¬ìš©ì„± ëª©í‘œ)
- [ ] ì§ê´€ì ì¸ ëª…ë ¹ì–´ ì¸í„°í˜ì´ìŠ¤ (ì‚¬ìš©ì„± ëª©í‘œ)

---

## ë¦¬ì†ŒìŠ¤ ë° ì°¸ê³ ìë£Œ

### í•„ìš”í•œ ë¦¬ì†ŒìŠ¤
- **ì¸ë ¥**: ë°±ì—”ë“œ ê°œë°œì 1ëª… (Python/SQLite/FastAPI ê²½í—˜)
- **ë„êµ¬**: SQLite, Qdrant, OpenAI API (ì„ë² ë”©), pytest, React
- **ì¸í”„ë¼**: ë¡œì»¬ ê°œë°œ í™˜ê²½, ë²¡í„° ì €ì¥ì†Œ

### í•™ìŠµ ìë£Œ
- [SQLite FTS5 ë¬¸ì„œ](https://sqlite.org/fts5.html) - ì „ë¬¸ ê²€ìƒ‰
- [Qdrant Python Client](https://qdrant.tech/documentation/quick-start/) - ë²¡í„° ê²€ìƒ‰
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings) - ì„ë² ë”© ìƒì„±
- [sentence-transformers](https://www.sbert.net/) - ë¡œì»¬ ì„ë² ë”© ëª¨ë¸

### ìœ ì‚¬ ì‚¬ë¡€
- Obsidianì˜ ë…¸íŠ¸ ì—°ê²° ì‹œìŠ¤í…œ
- Notionì˜ AI ê²€ìƒ‰ ê¸°ëŠ¥
- ChatGPTì˜ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
- Roam Researchì˜ ì–‘ë°©í–¥ ë§í¬

---

## ì§„í–‰ ìƒí™© ì¶”ì 

### Phase 1: ê¸°ë³¸ ì €ì¥ ì‹œìŠ¤í…œ (3ì¼)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- [ ] í”„ë¡œì íŠ¸ ì‹ë³„ ì‹œìŠ¤í…œ
- [ ] ê¸°ë³¸ ëŒ€í™” ì €ì¥ ë¡œì§
- [ ] JSON ë°±ì—… ì‹œìŠ¤í…œ

### Phase 2: ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ (2ì¼)
- [ ] ì¤‘ìš”ë„ ìë™ íŒì • ë¡œì§
- [ ] TTL ê¸°ë°˜ ìë™ ì‚­ì œ
- [ ] ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
- [ ] ì•ˆì „í•œ ì‚­ì œ í™•ì¸ ì‹œìŠ¤í…œ

### Phase 3: ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ (3ì¼)
- [ ] í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
- [ ] ì„ë² ë”© ìƒì„± ë° ì €ì¥
- [ ] Qdrant ë²¡í„° ì €ì¥ì†Œ ì—°ë™
- [ ] ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
- [ ] ì»¨í…ìŠ¤íŠ¸ ìë™ í¬í•¨ ë¡œì§

### Phase 4: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (2ì¼)
- [ ] AI CLI ë©”ëª¨ë¦¬ ëª…ë ¹ì–´
- [ ] Desktop App ë©”ëª¨ë¦¬ UI
- [ ] í†µê³„ ë° ì‹œê°í™”
- [ ] ì„¤ì • ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤

### Phase 5: ê³ ê¸‰ ê¸°ëŠ¥ (2ì¼)
- [ ] AI ìš”ì•½ ìƒì„±
- [ ] ì¤‘ìš” ì‚¬ì‹¤ ìë™ ì¶”ì¶œ
- [ ] ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

## ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. í ë“œë ˆì¸ ì›Œì»¤ë¥¼ APScheduler ë“± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ëŸ¬ë„ˆì— ì—°ê²°í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë¡œê¹…Â·ì•Œë¦¼ ë£¨í‹´ì„ êµ¬í˜„í•´ ë²¡í„° ë™ê¸°í™” ì¥ì• ë¥¼ ì¡°ê¸°ì— ê°ì§€í•œë‹¤.
2. `vector_sync_queue` ì²˜ë¦¬ì™€ TTL ê¸°ë°˜ ì‚­ì œ ë¡œì§ì„ ê²€ì¦í•˜ëŠ” ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì„¤ê³„í•´ ì¬ì‹œë„Â·ì‚­ì œ ë™ê¸°í™” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìë™í™”í•œë‹¤.

---

**ğŸ’¡ ì¶”ê°€ ê³ ë ¤ì‚¬í•­**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ í•„ìˆ˜ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)
- ì„ë² ë”© ìƒì„± ë¹„ìš© ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬, ìºì‹±)
- ì‚¬ìš©ì í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ (ë¡œì»¬ ì €ì¥ ì›ì¹™)
- ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ ëŒ€ì‘)
- Qdrant ì¥ì•  ëŒ€ë¹„ `vector_sync_queue` ê¸°ë°˜ ì¬ì‹œë„ ë° ì‚­ì œ ë™ê¸°í™” ì •ì±… ìœ ì§€

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-09-26
