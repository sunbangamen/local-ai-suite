# GitHub Issue Analysis & Solution Planning Command

**Usage:** `resolve-issue 1`

**Input:** GitHub Issue #1

---

## Step 1: Issue Retrieval & Analysis

### Fetch Issue Details
```bash
gh issue view 1 --json title,body,labels,assignees,milestone,state,createdAt,updatedAt
```

### Issue Information Summary
**ì´ìŠˆ ë²ˆí˜¸**: #1
**ì œëª©**: [Feature] Phase 1: ê¸°ë³¸ ëª¨ë¸ ì„œë¹™ + OpenAI í˜¸í™˜ API êµ¬ì¶•
**ìƒíƒœ**: Open
**ìƒì„±ì¼**: 2025-09-22T09:52:19Z
**ë‹´ë‹¹ì**: (ì—†ìŒ)
**ë¼ë²¨**: (ì—†ìŒ)
**ë§ˆì¼ìŠ¤í†¤**: (ì—†ìŒ)

### Issue Content Analysis
**ë¬¸ì œ ìœ í˜•**: Feature Implementation
**ìš°ì„ ìˆœìœ„**: High
**ë³µì¡ë„**: Medium

**í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
- ë¡œì»¬ GGUF ëª¨ë¸ì„ OpenAI í˜¸í™˜ APIë¡œ ì„œë¹™í•˜ëŠ” í™˜ê²½ êµ¬ì¶•
- `make up-p1` ëª…ë ¹ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì •ìƒ ì‹¤í–‰
- `curl http://localhost:8000/v1/models` ë° `/v1/chat/completions` API ì •ìƒ ë™ì‘
- VS Code/Cursorì—ì„œ `http://localhost:8000/v1` ì—°ê²° ì„±ê³µ
- RTX 4050 + WSL2 í™˜ê²½ì—ì„œ GPU í™œìš©

**ê¸°ìˆ ì  ì œì•½ì‚¬í•­**:
- Docker Compose ê¸°ë°˜ êµ¬í˜„
- llama.cpp (ì¶”ë¡  ì„œë²„) + LiteLLM (API ê²Œì´íŠ¸ì›¨ì´) ì‚¬ìš©
- GPU íŒ¨ìŠ¤ìŠ¤ë£¨ ì„¤ì • í•„ìš” (`device_requests` ì‚¬ìš©)
- ì™¸ì¥ SSD í™˜ê²½ ê³ ë ¤

---

## Step 2: Technical Investigation

### Code Analysis Required
```bash
# ê´€ë ¨ íŒŒì¼ë“¤ ê²€ìƒ‰
find . -name "*.yml" -o -name "*.yaml" -o -name "Makefile*" -o -name ".env*"
```

**ì˜í–¥ ë²”ìœ„ ë¶„ì„**:
- **Infrastructure**: Docker Compose ì„¤ì • ë° ì»¨í…Œì´ë„ˆ êµ¬ì„±
- **Networking**: í¬íŠ¸ 8000(API Gateway), 8001(Inference Server)
- **Storage**: ëª¨ë¸ íŒŒì¼ ë§ˆìš´íŠ¸ (`models/` ë””ë ‰í† ë¦¬)
- **GPU**: NVIDIA Docker runtime ì„¤ì •

### Dependency Check
**ì˜ì¡´ì„± ì´ìŠˆ**:
- Depends on: Docker Desktop + GPU ì§€ì›, GGUF ëª¨ë¸ íŒŒì¼
- Blocks: Phase 2 (RAG ì‹œìŠ¤í…œ), Phase 3 (MCP ì„œë²„)
- Related to: ì „ì²´ Local AI Suite í”„ë¡œì íŠ¸

**í˜„ì¬ í”„ë¡œì íŠ¸ ìƒíƒœ**:
- **Makefile**: Phase 1-3 ëª…ë ¹ì–´ ì´ë¯¸ ì •ì˜ë¨ (`make up-p1`)
- **.env.example**: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì¤€ë¹„ë¨
- **README.md**: ì „ì²´ ì•„í‚¤í…ì²˜ ë° ì‚¬ìš©ë²• ë¬¸ì„œí™”ë¨
- **Missing Files**: `docker/compose.p1.yml`, API Gateway ì„¤ì • íŒŒì¼

---

## Step 3: Solution Strategy

### Approach Options
**Option 1: llama.cpp + LiteLLM ì¡°í•© (ì¶”ì²œì•ˆ)**
- **ì¥ì **: ê²€ì¦ëœ ìŠ¤íƒ, OpenAI ì™„ë²½ í˜¸í™˜, GPU ìµœì í™”
- **ë‹¨ì **: ë‘ ê°œ ì»¨í…Œì´ë„ˆ ê´€ë¦¬ í•„ìš”
- **ì˜ˆìƒ ì‹œê°„**: 2-3ì‹œê°„
- **ìœ„í—˜ë„**: Low

**Option 2: vLLM ë‹¨ì¼ ì„œë²„**
- **ì¥ì **: ë‹¨ì¼ ì»¨í…Œì´ë„ˆ, ë†’ì€ ì„±ëŠ¥
- **ë‹¨ì **: GGUF ì§€ì› ì œí•œ, ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ë†’ìŒ
- **ì˜ˆìƒ ì‹œê°„**: 3-4ì‹œê°„
- **ìœ„í—˜ë„**: Medium

**Option 3: Ollama ê¸°ë°˜**
- **ì¥ì **: ì„¤ì • ê°„ë‹¨, ëª¨ë¸ ê´€ë¦¬ í¸ë¦¬
- **ë‹¨ì **: ë²„ì „ì— ë”°ë¼ OpenAI API í˜¸í™˜ ë²”ìœ„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (ì‚¬ì „ í™•ì¸ í•„ìš”)
- **ì˜ˆìƒ ì‹œê°„**: 1-2ì‹œê°„
- **ìœ„í—˜ë„**: Medium

### Recommended Approach
**ì„ íƒí•œ ì ‘ê·¼ë²•**: Option 1 - llama.cpp + LiteLLM ì¡°í•©
**ì„ íƒ ì´ìœ **: ì´ìŠˆ ëª…ì„¸ì„œì—ì„œ ìš”êµ¬í•œ ê¸°ìˆ  ìŠ¤íƒì´ë©°, RTX 4050 í™˜ê²½ì—ì„œ ê²€ì¦ëœ ì„±ëŠ¥ê³¼ OpenAI ì™„ë²½ í˜¸í™˜ì„± ì œê³µ

---

## Step 4: Detailed Implementation Plan

### Phase 0: ì‚¬ì „ì ê²€ (Preflight) (15ë¶„)
**ëª©í‘œ**: ì‹¤í–‰ í™˜ê²½ ë° ì˜ì¡´ì„± ì‚¬ì „ ê²€ì¦

| Task | Description | Owner | DoD | Risk |
|------|-------------|-------|-----|------|
| GPU í™˜ê²½ ê²€ì¦ | `nvidia-smi`, Docker GPU ì§€ì› í™•ì¸ | Dev | GPU ì¸ì‹ í™•ì¸ | High |
| ëª¨ë¸ íŒŒì¼ í™•ì¸ | GGUF ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ë° ì ‘ê·¼ ê¶Œí•œ í™•ì¸ | Dev | ëª¨ë¸ íŒŒì¼ ë¡œë”© ê°€ëŠ¥ | High |
| í¬íŠ¸ ì¶©ëŒ ê²€ì‚¬ | 8000, 8001 í¬íŠ¸ ì‚¬ìš© í˜„í™© í™•ì¸ | Dev | í¬íŠ¸ ì‚¬ìš© ê°€ëŠ¥ | Medium |
| í™˜ê²½ë³€ìˆ˜ ê²€ì¦ | `.env` íŒŒì¼ ë¡œë“œ ë° í•„ìˆ˜ ë³€ìˆ˜ í™•ì¸ | Dev | ì„¤ì •ê°’ ì •ìƒ | Low |

### Phase 1.1: í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„ (30ë¶„)
**ëª©í‘œ**: ê°œë°œ í™˜ê²½ ë° ê¸°ë³¸ ì„¤ì • ì™„ë£Œ

| Task | Description | Owner | DoD | Risk |
|------|-------------|-------|-----|------|
| ë¸Œëœì¹˜ ìƒì„± | `feature/phase1` ë¸Œëœì¹˜ ìƒì„± ë° ì²´í¬ì•„ì›ƒ | Dev | ë¸Œëœì¹˜ ìƒì„± ì™„ë£Œ | Low |
| í™˜ê²½íŒŒì¼ ìƒì„± | `.env.example` â†’ `.env` ë³µì‚¬ | Dev | `.env` íŒŒì¼ ìƒì„± | Low |
| ë””ë ‰í† ë¦¬ ìƒì„± | `docker/`, `services/api-gateway/` ìƒì„± | Dev | í•„ìš” ë””ë ‰í† ë¦¬ ì¡´ì¬ | Low |

### Phase 1.2: Docker ì„œë¹„ìŠ¤ êµ¬í˜„ (1.5ì‹œê°„)
**ëª©í‘œ**: ì»¨í…Œì´ë„ˆ ì„¤ì • ë° ì„œë¹„ìŠ¤ ì •ì˜

| Task | Description | Owner | DoD | Risk |
|------|-------------|-------|-----|------|
| Docker Compose ì‘ì„± | `docker/compose.p1.yml` ìƒì„± | Dev | ì¶”ë¡ ì„œë²„+ê²Œì´íŠ¸ì›¨ì´ ì •ì˜ | Medium |
| API Gateway ì„¤ì • | LiteLLM ì„¤ì •íŒŒì¼ ì‘ì„± | Dev | OpenAI í˜¸í™˜ ì„¤ì • ì™„ë£Œ | Medium |
| GPU ì„¤ì • êµ¬ì„± | device_requestsë¡œ GPU íŒ¨ìŠ¤ìŠ¤ë£¨ | Dev | GPU ì¸ì‹ í™•ì¸ | High |
| ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì„¤ì • | ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • | Dev | ëª¨ë¸ ë¡œë”© í™•ì¸ | Medium |

### Phase 1.3: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (1ì‹œê°„)
**ëª©í‘œ**: ì „ì²´ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸

| Task | Description | Owner | DoD | Risk |
|------|-------------|-------|-----|------|
| ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ | `make up-p1` ì‹¤í–‰ í™•ì¸ | Dev | ëª¨ë“  ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹¤í–‰ | Medium |
| API ì‘ë‹µ í…ŒìŠ¤íŠ¸ | `/v1/models`, `/v1/chat/completions` í…ŒìŠ¤íŠ¸ | Dev | ì •ìƒ ì‘ë‹µ í™•ì¸ | Low |
| IDE ì—°ë™ í…ŒìŠ¤íŠ¸ | VS Code/Cursor ì—°ê²° í…ŒìŠ¤íŠ¸ | Dev | AI ì‘ë‹µ ìƒì„± í™•ì¸ | Low |

---

## Step 5: Risk Assessment & Mitigation

### High Risk Items
| Risk | Impact | Probability | Mitigation Strategy |
|------|--------|-------------|-------------------|
| GPU ì¸ì‹ ì‹¤íŒ¨ | High | Medium | Docker Desktop GPU ì„¤ì • í™•ì¸, nvidia-docker2 ì„¤ì¹˜ ê²€ì¦ |
| ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ | High | High | ëª¨ë¸ íŒŒì¼ ê²½ë¡œ/ê¶Œí•œ í™•ì¸, GGUF í¬ë§· ê²€ì¦ |
| ë©”ëª¨ë¦¬ ë¶€ì¡± | Medium | Medium | 4K context ì œí•œ, Q4 ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© |

### Technical Challenges
**ì˜ˆìƒ ê¸°ìˆ ì  ë‚œì **:
1. **GPU íŒ¨ìŠ¤ìŠ¤ë£¨ ì„¤ì •** - WSL2 + Docker Desktop í™˜ê²½ì—ì„œ `device_requests` ì„¤ì •
2. **í¬íŠ¸ ë°”ì¸ë”©** - 8000, 8001 í¬íŠ¸ ì¶©ëŒ ë°©ì§€
3. **ëª¨ë¸ ë¡œë”© ì‹œê°„** - ì²« ìš”ì²­ ì‹œ ìˆ˜ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŒ

### Rollback Plan
**ë¡¤ë°± ì‹œë‚˜ë¦¬ì˜¤**:
- GPU ì‹¤íŒ¨ ì‹œ â†’ CPU ëª¨ë“œë¡œ í´ë°± (`CUDA_VISIBLE_DEVICES=""`)
- ì»¨í…Œì´ë„ˆ ì‹¤íŒ¨ ì‹œ â†’ `make down` í›„ ì¬ì‹œì‘

---

## Step 6: Resource Requirements

### Human Resources
- **ê°œë°œì**: 1ëª… (Docker, GPU ì„¤ì • ê²½í—˜ í•„ìš”)
- **ë¦¬ë·°ì–´**: 1ëª… (ì¸í”„ë¼ ê²€ì¦)

### Technical Resources
- **ê°œë°œ ë„êµ¬**: Docker Desktop, NVIDIA Container Toolkit
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: RTX 4050 + WSL2
- **ëª¨ë¸ íŒŒì¼**: GGUF í¬ë§· 7B ëª¨ë¸ (~4GB)

### Time Estimation
- **ì‚¬ì „ì ê²€**: 15ë¶„
- **í•µì‹¬ êµ¬í˜„**: 2.5ì‹œê°„
- **í…ŒìŠ¤íŠ¸ & ê²€ì¦**: 1ì‹œê°„
- **ì´ ì˜ˆìƒ ì‹œê°„**: 3.75ì‹œê°„
- **ë²„í¼ ì‹œê°„**: 1.25ì‹œê°„ (33% ë²„í¼)
- **ì™„ë£Œ ëª©í‘œì¼**: 2025-09-22

---

## Step 7: Quality Assurance Plan

### Test Strategy
**í…ŒìŠ¤íŠ¸ ë ˆë²¨**:
- **Integration Tests**: ì»¨í…Œì´ë„ˆ ê°„ í†µì‹  ë° API í˜¸ì¶œ
- **System Tests**: ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
- **Compatibility Tests**: IDE ì—°ë™ í…ŒìŠ¤íŠ¸

### Test Cases
```gherkin
Feature: Phase 1 ê¸°ë³¸ ëª¨ë¸ ì„œë¹™

Scenario: ì„œë¹„ìŠ¤ ì •ìƒ ì‹¤í–‰
  Given Docker Desktopì´ ì‹¤í–‰ ì¤‘ì´ê³ 
  And ëª¨ë¸ íŒŒì¼ì´ models/ ë””ë ‰í† ë¦¬ì— ìˆì„ ë•Œ
  When make up-p1ì„ ì‹¤í–‰í•˜ë©´
  Then ëª¨ë“  ì»¨í…Œì´ë„ˆê°€ ì •ìƒ ì‹¤í–‰ëœë‹¤

Scenario: API í˜¸ì¶œ ì„±ê³µ
  Given Phase 1 ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œ
  When /v1/models APIë¥¼ í˜¸ì¶œí•˜ë©´
  Then ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤
  And /v1/chat/completions API í˜¸ì¶œ ì‹œ
  Then AI ì‘ë‹µì„ ì •ìƒ ìƒì„±í•œë‹¤

Scenario: IDE ì—°ë™ ì„±ê³µ
  Given API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œ
  When VS Codeì—ì„œ http://localhost:8000/v1ë¡œ ì—°ê²°í•˜ë©´
  Then AI ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ë™ì‘í•œë‹¤
```

### Performance Criteria
- **ì‘ë‹µì‹œê°„**: ì²« ìš”ì²­ < 30ì´ˆ, ì´í›„ ìš”ì²­ < 5ì´ˆ
- **ì²˜ë¦¬ëŸ‰**: ì—°ì† ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
- **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ **: GPU ë©”ëª¨ë¦¬ < 6GB, CPU < 70%

---

## Step 8: Communication Plan

### Status Updates
- **ì´ìŠˆ ëŒ“ê¸€ ì—…ë°ì´íŠ¸**: ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ë§ˆë‹¤ ì§„í–‰ìƒí™© ê³µìœ 
- **Git ì»¤ë°‹**: ë‹¨ê³„ë³„ ì‘ì—… ì™„ë£Œ ì‹œì ì— ì»¤ë°‹

### Stakeholder Notification
- **GitHub Issue**: êµ¬í˜„ ì™„ë£Œ ì‹œ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
- **README**: ì„±ê³µì  êµ¬í˜„ ì‹œ ì‚¬ìš©ë²• ê°€ì´ë“œ ì—…ë°ì´íŠ¸

---

## ğŸ“‹ êµ¬í˜„í•  íŒŒì¼ ëª©ë¡

### ì‚¬ì „ì ê²€ ìŠ¤í¬ë¦½íŠ¸

**scripts/preflight.sh**
```bash
#!/usr/bin/env bash
set -e
echo "[1/4] GPU ì²´í¬"; docker run --rm --gpus all nvidia/cuda:12.2-base-ubuntu20.04 nvidia-smi >/dev/null && echo "âœ… GPU ì¸ì‹ ì„±ê³µ"
echo "[2/4] ëª¨ë¸ íŒŒì¼ ì²´í¬"; test -f "./models/${CHAT_MODEL}" && echo "âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬" || (echo "âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: ${CHAT_MODEL}"; exit 1)
echo "[3/4] í¬íŠ¸ ì²´í¬"; nc -z localhost ${API_GATEWAY_PORT:-8000} 2>/dev/null && echo "âš ï¸  í¬íŠ¸ ${API_GATEWAY_PORT:-8000} ì‚¬ìš©ì¤‘" || echo "âœ… í¬íŠ¸ ${API_GATEWAY_PORT:-8000} ì‚¬ìš© ê°€ëŠ¥"
nc -z localhost ${INFERENCE_PORT:-8001} 2>/dev/null && echo "âš ï¸  í¬íŠ¸ ${INFERENCE_PORT:-8001} ì‚¬ìš©ì¤‘" || echo "âœ… í¬íŠ¸ ${INFERENCE_PORT:-8001} ì‚¬ìš© ê°€ëŠ¥"
echo "[4/4] .env ê²€ì¦"; source .env && grep -E "CHAT_MODEL|PORT" .env && echo "âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì„±ê³µ"
echo "ğŸš€ Preflight ì ê²€ ì™„ë£Œ"
```

### ìƒì„±í•  íŒŒì¼ë“¤

1. **docker/compose.p1.yml**
```yaml
version: "3.9"
services:
  inference:
    image: ghcr.io/ggerganov/llama.cpp:full
    command: [
      "--server", "--host", "0.0.0.0", "--port", "8001",
      "--model", "/models/${CHAT_MODEL}",
      "--parallel", "4", "--ctx-size", "8192",
      "--n-gpu-layers", "35"
    ]
    volumes:
      - ../models:/models:ro
    ports:
      - "${INFERENCE_PORT:-8001}:8001"
    device_requests:
      - driver: "nvidia"
        count: -1
        capabilities: [["gpu"]]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  api-gateway:
    image: ghcr.io/berriai/litellm:latest
    environment:
      - LITELLM_CONFIG=/app/config.yaml
      - LITELLM_LOG=INFO
    volumes:
      - ../services/api-gateway/config.p1.yaml:/app/config.yaml:ro
    ports:
      - "${API_GATEWAY_PORT:-8000}:8000"
    depends_on:
      inference:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
```

2. **services/api-gateway/config.p1.yaml**
```yaml
model_list:
  - model_name: "local-chat"
    litellm_params:
      model: "llamacpp/chat"  # llama.cpp ì „ìš© í”„ë¡œë°”ì´ë”
      api_base: "http://inference:8001"
      api_key: "dummy-key"  # ë¡œì»¬ í™˜ê²½ìš©
      temperature: 0.2
      max_tokens: 2048

defaults:
  temperature: 0.2
  max_tokens: 1024
  timeout: 120

server:
  host: 0.0.0.0
  port: 8000

logging:
  level: INFO
```

3. **í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env)**
```bash
# ë³µì‚¬ ëª…ë ¹: cp .env.example .env
# í•„ìˆ˜ ì„¤ì •ê°’ í™•ì¸
API_GATEWAY_PORT=8000
INFERENCE_PORT=8001
CHAT_MODEL=llama3.1-8b-instruct-q4_k_m.gguf
CODE_MODEL=qwen2.5-coder-7b-q4_k_m.gguf

# GPU ì„¤ì • (í•„ìš”ì‹œ ì¡°ì •)
CUDA_VISIBLE_DEVICES=0
```

---

## ğŸ“‹ User Review Checklist

**ë‹¤ìŒ í•­ëª©ë“¤ì„ ê²€í† í•´ì£¼ì„¸ìš”:**

### Planning Review
- [x] **ì´ìŠˆ ë¶„ì„ì´ ì •í™•í•œê°€ìš”?**
  - í•µì‹¬ ìš”êµ¬ì‚¬í•­: ë¡œì»¬ GGUF ëª¨ë¸ â†’ OpenAI API ì„œë¹™
  - ê¸°ìˆ ì  ì œì•½ì‚¬í•­: Docker + GPU + llama.cpp + LiteLLM

- [x] **ì„ íƒí•œ í•´ê²° ë°©ì•ˆì´ ì ì ˆí•œê°€ìš”?**
  - llama.cpp + LiteLLM ì¡°í•©ì€ ì´ìŠˆì—ì„œ ìš”êµ¬í•œ ì •í™•í•œ ìŠ¤íƒ
  - ê²€ì¦ëœ ë°©ì‹ì´ë©° RTX 4050 í™˜ê²½ì— ìµœì í™”ë¨

- [x] **êµ¬í˜„ ê³„íšì´ í˜„ì‹¤ì ì¸ê°€ìš”?**
  - 3ê°œ Phaseë¡œ ë‹¨ê³„ë³„ êµ¬í˜„ (í™˜ê²½ì„¤ì • â†’ êµ¬í˜„ â†’ í…ŒìŠ¤íŠ¸)
  - ê° ë‹¨ê³„ë³„ ëª…í™•í•œ DoDì™€ ìœ„í—˜ë„ í‰ê°€

### Resource Review
- [x] **ì‹œê°„ ì¶”ì •ì´ í•©ë¦¬ì ì¸ê°€ìš”?**
  - ì´ 3-4ì‹œê°„ì€ Docker + GPU ì„¤ì • ê²½í—˜ ê¸°ì¤€ìœ¼ë¡œ ì ì ˆ
  - 33% ë²„í¼ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì´ìŠˆ ëŒ€ì‘ ê°€ëŠ¥

- [x] **í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ê°€ í™•ë³´ ê°€ëŠ¥í•œê°€ìš”?**
  - Docker Desktop, GPU ë“œë¼ì´ë²„ëŠ” ì´ë¯¸ ì„¤ì¹˜ ì „ì œ
  - GGUF ëª¨ë¸ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ í•„ìš”

### Risk Review
- [x] **ìœ„í—˜ ìš”ì†Œê°€ ì¶©ë¶„íˆ ì‹ë³„ë˜ì—ˆë‚˜ìš”?**
  - GPU ì¸ì‹ ì‹¤íŒ¨: WSL2 í™˜ê²½ì˜ ëŒ€í‘œì  ì´ìŠˆ
  - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: íŒŒì¼ ê²½ë¡œ/ê¶Œí•œ ë¬¸ì œ ìì£¼ ë°œìƒ
  - ë©”ëª¨ë¦¬ ë¶€ì¡±: RTX 4050 6GB ì œí•œ ê³ ë ¤

- [x] **ë¡¤ë°± ê³„íšì´ í˜„ì‹¤ì ì¸ê°€ìš”?**
  - GPU ì‹¤íŒ¨ ì‹œ CPU í´ë°±ì€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
  - `make down`ìœ¼ë¡œ ì™„ì „ ì •ë¦¬ í›„ ì¬ì‹œì‘ ê°€ëŠ¥

### Quality Review
- [x] **í…ŒìŠ¤íŠ¸ ì „ëµì´ ì¶©ë¶„í•œê°€ìš”?**
  - ì»¨í…Œì´ë„ˆ ì‹¤í–‰ â†’ API í˜¸ì¶œ â†’ IDE ì—°ë™ê¹Œì§€ ì „ì²´ í”Œë¡œìš° ì»¤ë²„
  - ì„±ëŠ¥/ì•ˆì •ì„± ê¸°ì¤€ë„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ì •

---

## ğŸš€ Next Steps

**ê²€í†  ì™„ë£Œ í›„ ì§„í–‰í•  ì‘ì—…:**

1. **Plan Approval**: ìœ„ ê²€í† ë¥¼ í†µê³¼í•˜ë©´ ê³„íš ìŠ¹ì¸
2. **Issue Update**: GitHub ì´ìŠˆì— ìƒì„¸ ê³„íš ëŒ“ê¸€ë¡œ ì¶”ê°€
3. **Branch Creation**: `feature/phase1` ë¸Œëœì¹˜ ìƒì„±
4. **Implementation**: ë‹¨ê³„ë³„ íŒŒì¼ ìƒì„± ë° êµ¬í˜„
5. **Testing**: ê° ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

**êµ¬í˜„ ìˆœì„œ:**
0. **ì‚¬ì „ì ê²€ ì‹¤í–‰**: `bash scripts/preflight.sh`
1. **í™˜ê²½ ì„¤ì •**: `.env` ìƒì„±, ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
2. **Docker Compose ì‘ì„±**: GPU íŒ¨ìŠ¤ìŠ¤ë£¨ í¬í•¨í•œ ì™„ì„±ëœ ì„¤ì •
3. **LiteLLM ì„¤ì •**: OpenAI í˜¸í™˜ API ê²Œì´íŠ¸ì›¨ì´ êµ¬ì„±
4. **ì„œë¹„ìŠ¤ ì‹¤í–‰**: `make up-p1` ë° í—¬ìŠ¤ì²´í¬ í™•ì¸
5. **API í…ŒìŠ¤íŠ¸**: `/v1/models`, `/v1/chat/completions` ê²€ì¦
6. **IDE ì—°ë™ í…ŒìŠ¤íŠ¸**: VS Code/Cursor ì—°ê²° í™•ì¸
7. **ë¬¸ì œ ë°œìƒ ì‹œ**: ë¡œê·¸ í™•ì¸ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**í•µì‹¬ ì‹¤í–‰ ëª…ë ¹ì–´:**
```bash
# ì‚¬ì „ì ê²€
bash scripts/preflight.sh

# ì„œë¹„ìŠ¤ ì‹¤í–‰
make up-p1

# API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/v1/models
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "local-chat", "messages": [{"role": "user", "content": "Hello!"}], "max_tokens": 50}'

# ì„œë¹„ìŠ¤ ì¤‘ì§€
make down
```

---

**ğŸ’¡ í”¼ë“œë°± ìš”ì²­**
ì´ ê³„íšì— ëŒ€í•´ ì–´ë–¤ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ë³´ì™„í•´ì•¼ í• ê¹Œìš”? íŠ¹íˆ:
- GPU ì„¤ì • ë¶€ë¶„ì—ì„œ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ì´ ìˆëŠ”ì§€?
- ì‹œê°„ ì¶”ì •ì´ í˜„ì‹¤ì ì¸ì§€?
- ëˆ„ë½ëœ ìœ„í—˜ ìš”ì†Œë‚˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆëŠ”ì§€?

**ì£¼ì˜:** PR ìƒì„± ë° ë³‘í•©ì€ ìë™ìœ¼ë¡œ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
í•„ìš” ì‹œ ì‚¬ìš©ìê°€ ì§ì ‘ `gh pr create` ë“±ì˜ ëª…ë ¹ìœ¼ë¡œ ìˆ˜ë™ ì§„í–‰í•˜ì„¸ìš”.