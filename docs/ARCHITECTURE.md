# ğŸ“¦ Local AI Suite â€” ë‹¨ê³„ë³„ ìŠ¤ìºí´ë“œ (Claude Code & Codex ì¹œí™”í˜•)

> ì™¸ì¥ SSD(1TB) + RTX 4050 ê¸°ì¤€.
> **Phase 1 â†’ 3** ìˆœì„œë¡œ ì ì§„ êµ¬ì¶•. ê° PhaseëŠ” ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥.
>
> âœ… *ëª¨ë“  íŒŒì¼ì€ ì´ ë¬¸ì„œì˜ ê²½ë¡œ/ë‚´ìš©ëŒ€ë¡œ ë³µë¶™í•˜ë©´ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆê²Œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.*

---

## ğŸ—‚ï¸ 0. ë¦¬í¬ì§€í† ë¦¬ êµ¬ì¡° (ìµœì¢…)

```
local-ai-suite/
â”œâ”€ README.md
â”œâ”€ Makefile
â”œâ”€ .env.example
â”œâ”€ docker/
â”‚  â”œâ”€ compose.p1.yml   # Phase 1 (ëª¨ë¸ + OpenAI í˜¸í™˜ ê²Œì´íŠ¸ì›¨ì´)
â”‚  â”œâ”€ compose.p2.yml   # Phase 2 (RAG + Qdrant + reranker ì¶”ê°€)
â”‚  â””â”€ compose.p3.yml   # Phase 3 (MCP ì„œë²„ ì¶”ê°€)
â”œâ”€ services/
â”‚  â”œâ”€ api-gateway/
â”‚  â”‚  â”œâ”€ config.p1.yaml
â”‚  â”‚  â””â”€ config.p2.yaml
â”‚  â”œâ”€ inference/        # (ì„ íƒ) ë¡œì»¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸/í—¬í¼
â”‚  â”œâ”€ rag/
â”‚  â”‚  â”œâ”€ Dockerfile
â”‚  â”‚  â”œâ”€ requirements.txt
â”‚  â”‚  â””â”€ app.py
â”‚  â”œâ”€ reranker/
â”‚  â”‚  â”œâ”€ Dockerfile
â”‚  â”‚  â”œâ”€ requirements.txt
â”‚  â”‚  â””â”€ serve_reranker.py
â”‚  â””â”€ mcp/
â”‚     â”œâ”€ Dockerfile
â”‚     â”œâ”€ requirements.txt
â”‚     â””â”€ servers/
â”‚        â”œâ”€ fs_server.py
â”‚        â”œâ”€ git_server.py
â”‚        â””â”€ shell_server.py
â”œâ”€ models/             # GGUF/OLLAMA ìºì‹œ(ì™¸ì¥ SSD)
â””â”€ data/
   â”œâ”€ rag_docs/        # PDF/MD/ì½”ë“œ/ë¬¸ì„œ ì›ë³¸
   â”œâ”€ chunks/
   â”œâ”€ indices/
   â””â”€ cache/
```

---

## ğŸ“˜ 1) README.md (ë¦¬í¬ ìµœìƒë‹¨)

**`README.md`**

````markdown
# Local AI Suite (Phase-by-Phase)

ì™¸ì¥ SSD + RTX 4050ì—ì„œ **í´ë¡œë“œ ë°ìŠ¤í¬íƒ‘/ì½”ë“œ/ì»¤ì„œ ëŠë‚Œ**ì„ ë¡œì»¬ ëª¨ë¸ + RAG + MCPë¡œ êµ¬í˜„í•˜ëŠ” ìŠ¤ìºí´ë“œ.

## Quick Start

### 0) ì‚¬ì „ ì¤€ë¹„
- Docker Desktop + WSL í†µí•©(Windows)
- ì™¸ì¥ SSDì— ì´ ë¦¬í¬ì§€í† ë¦¬ í´ë¡  í›„ `models/` í´ë” ìƒì„±
- 7B GGUF ëª¨ë¸ íŒŒì¼ì„ `models/`ì— ë°°ì¹˜(ì˜ˆ: llama3.1-8b-instruct-q4_k_m.gguf, qwen2.5-coder-7b-q4_k_m.gguf)

### 1) Phase 1: ìµœì†Œ ë™ì‘ (ëª¨ë¸ + OpenAI í˜¸í™˜ ê²Œì´íŠ¸ì›¨ì´)
```bash
make up-p1
# í™•ì¸
curl http://localhost:8000/v1/models
````

* VS Code/Cursorì—ì„œ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ë¥¼ `http://localhost:8000/v1` ë¡œ ì„¤ì •

### 2) Phase 2: RAG + Qdrant + reranker ì¶”ê°€

```bash
make up-p2
# ë¬¸ì„œ ì¸ë±ì‹±
curl -X POST "http://localhost:8002/index?collection=myproj"
# ì§ˆì˜
curl -H "Content-Type: application/json" \
     -d '{"query":"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì›ì¸ ì •ë¦¬","collection":"myproj"}' \
     http://localhost:8002/query
```

### 3) Phase 3: MCP ì„œë²„

```bash
make up-p3
# MCP(íŒŒì¼/ê¹ƒ/ì…¸) ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
curl http://localhost:8020/health
```

## í´ë” ìš”ì•½

* `docker/compose.p1.yml` : ì¶”ë¡ ì„œë²„ + API ê²Œì´íŠ¸ì›¨ì´(litellm)
* `docker/compose.p2.yml` : + Qdrant + RAG(FastAPI) + reranker
* `docker/compose.p3.yml` : + MCP ì„œë²„(fs/git/shell)

## ë³´ì•ˆ

* ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” ë¡œì»¬í˜¸ìŠ¤íŠ¸ë§Œ ë…¸ì¶œ ê¶Œì¥.
* ì™¸ë¶€ í¬íŠ¸ ê°œë°© ê¸ˆì§€. í† í°/í‚¤ í•„ìš” ì—†ìŒ(ì™„ì „ ë¡œì»¬ ì „ì œ).

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

* ëª¨ë¸ ê²½ë¡œ/íŒŒì¼ëª… ì˜¤íƒ€ â†’ `docker logs`ì—ì„œ í™•ì¸
* GPU ì¸ì‹ ì•ˆë  ë•Œ â†’ Docker Desktopì—ì„œ WSL GPU ì§€ì›/ë“œë¼ì´ë²„ í™•ì¸
* RAG í’ˆì§ˆì´ ë‚®ì„ ë•Œ â†’ bge-m3 ì„ë² ë”©, bge-reranker ì„¤ì • í™•ì¸

```
```

---

## âš™ï¸ 2) Makefile

**`Makefile`**

```make
.PHONY: up-p1 up-p2 up-p3 down logs

up-p1:
	docker compose -f docker/compose.p1.yml --env-file .env up -d --build

up-p2:
	docker compose -f docker/compose.p2.yml --env-file .env up -d --build

up-p3:
	docker compose -f docker/compose.p3.yml --env-file .env up -d --build

down:
	docker compose -f docker/compose.p3.yml down || true
	docker compose -f docker/compose.p2.yml down || true
	docker compose -f docker/compose.p1.yml down || true

logs:
	docker compose -f docker/compose.p3.yml logs -f || \
	docker compose -f docker/compose.p2.yml logs -f || \
	docker compose -f docker/compose.p1.yml logs -f
```

---

## ğŸ” 3) .env.example

**`.env.example`**

```dotenv
# ê³µí†µ
HOST=0.0.0.0
API_GATEWAY_PORT=8000
INFERENCE_PORT=8001
RAG_PORT=8002
RERANKER_PORT=8003
MCP_PORT=8020

# ëª¨ë¸ íŒŒì¼ëª…(ì‹¤ì œ íŒŒì¼ì€ models/ í´ë”ì— ë‘ )
CHAT_MODEL=llama3.1-8b-instruct-q4_k_m.gguf
CODE_MODEL=qwen2.5-coder-7b-q4_k_m.gguf

# RAG
QDRANT_URL=http://qdrant:6333
EMBEDDING_MODEL=bge-m3
RERANKER_URL=http://reranker:8003
```

> `.env`ëŠ” `.env.example`ì„ ë³µì‚¬í•´ ì±„ìš°ì„¸ìš”.

---

## ğŸ³ 4) docker/compose.p1.yml (Phase 1)

**`docker/compose.p1.yml`**

```yaml
version: "3.9"
services:
  inference:
    image: ghcr.io/ggerganov/llama.cpp:full
    command: ["--server", "--host", "0.0.0.0", "--port", "8001",
              "--model", "/models/${CHAT_MODEL}",
              "--parallel", "4", "--ctx-size", "8192"]
    volumes:
      - ../models:/models
    ports:
      - "${INFERENCE_PORT:-8001}:8001"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  api-gateway:
    image: ghcr.io/berriai/litellm:latest
    environment:
      - LITELLM_CONFIG=/app/config.yaml
    volumes:
      - ../services/api-gateway/config.p1.yaml:/app/config.yaml
    ports:
      - "${API_GATEWAY_PORT:-8000}:8000"
    depends_on:
      - inference
```

---

## ğŸ”Œ 5) services/api-gateway/config.p1.yaml (Phase 1)

**`services/api-gateway/config.p1.yaml`**

```yaml
model_list:
  - model_name: "local-chat"
    litellm_params:
      model: "openai/generic"
      api_base: "http://inference:8001/v1"
      api_key: "none"

defaults:
  temperature: 0.2
  max_tokens: 1024

server:
  host: 0.0.0.0
  port: 8000
```

> VS Code/CursorëŠ” `http://localhost:8000/v1` ë¡œ ì—°ê²°í•˜ì„¸ìš”.

---

## ğŸ³ 6) docker/compose.p2.yml (Phase 2)

**`docker/compose.p2.yml`**

```yaml
version: "3.9"
services:
  inference:
    image: ghcr.io/ggerganov/llama.cpp:full
    command: ["--server", "--host", "0.0.0.0", "--port", "8001",
              "--model", "/models/${CHAT_MODEL}",
              "--parallel", "4", "--ctx-size", "8192"]
    volumes:
      - ../models:/models
    ports:
      - "${INFERENCE_PORT:-8001}:8001"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  qdrant:
    image: qdrant/qdrant:v1.11.0
    volumes:
      - ./qdrant:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"

  reranker:
    build: ../services/reranker
    ports:
      - "${RERANKER_PORT:-8003}:8003"
    depends_on: [qdrant]

  rag:
    build: ../services/rag
    env_file: ../.env
    volumes:
      - ../data:/data
    ports:
      - "${RAG_PORT:-8002}:8002"
    depends_on: [qdrant, reranker]

  api-gateway:
    image: ghcr.io/berriai/litellm:latest
    environment:
      - LITELLM_CONFIG=/app/config.yaml
    volumes:
      - ../services/api-gateway/config.p2.yaml:/app/config.yaml
    ports:
      - "${API_GATEWAY_PORT:-8000}:8000"
    depends_on:
      - inference
      - rag
```

---

## ğŸ”Œ 7) services/api-gateway/config.p2.yaml (Phase 2)

**`services/api-gateway/config.p2.yaml`**

```yaml
model_list:
  - model_name: "local-chat"
    litellm_params:
      model: "openai/generic"
      api_base: "http://inference:8001/v1"
      api_key: "none"

rag_providers:
  - name: "rag"
    base_url: "http://rag:8002"

defaults:
  temperature: 0.2
  max_tokens: 1024

server:
  host: 0.0.0.0
  port: 8000
```

---

## ğŸ§  8) services/rag/Dockerfile

**`services/rag/Dockerfile`**

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY app.py ./
EXPOSE 8002
CMD ["python", "app.py"]
```

### services/rag/requirements.txt

**`services/rag/requirements.txt`**

```txt
fastapi==0.112.0
uvicorn[standard]==0.30.5
httpx==0.27.0
qdrant-client==1.9.2
pydantic==2.8.2
python-multipart==0.0.9
nltk==3.9.1
sentence-transformers==3.0.1
```

### services/rag/app.py (ë¯¸ë‹ˆë©€ RAG API)

**`services/rag/app.py`**

```python
from fastapi import FastAPI, UploadFile, File, Query
from pydantic import BaseModel
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm
import os, glob, json

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3")
RERANKER_URL = os.getenv("RERANKER_URL", "http://localhost:8003")
DOC_DIR = "/data/rag_docs"

app = FastAPI(title="RAG Service")

# --- ë§¤ìš° ë‹¨ìˆœí•œ ì„ë² ë”© placeholder (ì‹¤ì „ì—ì„œëŠ” bge-m3 ë¡œ êµì²´) ---
from sentence_transformers import SentenceTransformer
_embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

qdrant = QdrantClient(url=QDRANT_URL)

class QueryBody(BaseModel):
    query: str
    collection: str
    top_k: int = 5

@app.post("/index")
def index_collection(collection: str = Query(...)):
    # 1) ì»¬ë ‰ì…˜ ìƒì„±(ì—†ìœ¼ë©´)
    qdrant.recreate_collection(
        collection_name=collection,
        vectors_config=qm.VectorParams(size=_embedder.get_sentence_embedding_dimension(), distance=qm.Distance.COSINE),
    )
    # 2) íŒŒì¼ ë¡œë“œ & ë¶„í• (ê°„ë‹¨)
    docs = []
    for path in glob.glob(f"{DOC_DIR}/**/*", recursive=True):
        if os.path.isfile(path) and not os.path.basename(path).startswith('.'):
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
                if text.strip():
                    docs.append({"id": path, "text": text[:5000]})  # ë°ëª¨: ì•ë¶€ë¶„ë§Œ
            except Exception:
                pass
    # 3) ì„ë² ë”© & ì—…ì„œíŠ¸
    payloads = []
    vectors = []
    ids = []
    for d in docs:
        vec = _embedder.encode(d["text"]).tolist()
        ids.append(d["id"])  # pathë¥¼ idë¡œ
        vectors.append(vec)
        payloads.append({"path": d["id"], "preview": d["text"][:200]})

    if vectors:
        qdrant.upsert(collection_name=collection, points=qm.Batch(ids=ids, vectors=vectors, payloads=payloads))
    return {"ok": True, "count": len(vectors)}

@app.post("/query")
def query_rag(body: QueryBody):
    q = body.query
    q_vec = _embedder.encode(q).tolist()
    res = qdrant.search(collection_name=body.collection, query_vector=q_vec, limit=body.top_k)
    # (ì„ íƒ) reranker í˜¸ì¶œ ìƒëµ â€” ë°ëª¨ ë‹¨ìˆœí™”
    contexts = [
        {
            "score": float(hit.score),
            "path": hit.payload.get("path"),
            "preview": hit.payload.get("preview"),
        }
        for hit in res
    ]
    return {"query": q, "contexts": contexts}
```

> ì‹¤ì „ì—ì„œëŠ” bge-m3 ì„ë² ë”© + reranker(ì•„ë˜)ë¡œ ê³ ë„í™”í•˜ì„¸ìš”. ì—¬ê¸°ì„  Phase 2 ë°ëª¨ë¥¼ ìœ„í•´ ê°„ì†Œí™”.

---

## ğŸ§® 9) services/reranker

**`services/reranker/Dockerfile`**

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY serve_reranker.py ./
EXPOSE 8003
CMD ["python", "serve_reranker.py"]
```

**`services/reranker/requirements.txt`**

```txt
fastapi==0.112.0
uvicorn[standard]==0.30.5
scikit-learn==1.5.1
numpy==1.26.4
```

**`services/reranker/serve_reranker.py`**

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Reranker Stub")

class Item(BaseModel):
    text: str
    score: float

class RankBody(BaseModel):
    query: str
    items: list[Item]

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/rank")
def rank(body: RankBody):
    # ë°ëª¨: ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœë§Œ (ì‹¤ì „ì—” bge-reranker ì ìš©)
    ranked = sorted(body.items, key=lambda x: x.score, reverse=True)
    return {"items": [i.model_dump() for i in ranked]}
```

---

## ğŸ§° 10) docker/compose.p3.yml (Phase 3: MCP)

**`docker/compose.p3.yml`**

```yaml
version: "3.9"
services:
  inference:
    image: ghcr.io/ggerganov/llama.cpp:full
    command: ["--server", "--host", "0.0.0.0", "--port", "8001",
              "--model", "/models/${CHAT_MODEL}",
              "--parallel", "4", "--ctx-size", "8192"]
    volumes:
      - ../models:/models
    ports:
      - "${INFERENCE_PORT:-8001}:8001"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  qdrant:
    image: qdrant/qdrant:v1.11.0
    volumes:
      - ./qdrant:/qdrant/storage
    ports: ["6333:6333", "6334:6334"]

  reranker:
    build: ../services/reranker
    ports: ["${RERANKER_PORT:-8003}:8003"]
    depends_on: [qdrant]

  rag:
    build: ../services/rag
    env_file: ../.env
    volumes:
      - ../data:/data
    ports: ["${RAG_PORT:-8002}:8002"]
    depends_on: [qdrant, reranker]

  api-gateway:
    image: ghcr.io/berriai/litellm:latest
    environment:
      - LITELLM_CONFIG=/app/config.yaml
    volumes:
      - ../services/api-gateway/config.p2.yaml:/app/config.yaml
    ports: ["${API_GATEWAY_PORT:-8000}:8000"]
    depends_on: [inference, rag]

  mcp:
    build: ../services/mcp
    volumes:
      - ../services/mcp:/app
      - ..:/workspace
    ports: ["${MCP_PORT:-8020}:8020"]
```

---

## ğŸ§© 11) services/mcp (fs/git/shell â€” ë°ëª¨)

**`services/mcp/Dockerfile`**

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY servers/ ./servers/
EXPOSE 8020
CMD ["python", "servers/fs_server.py"]
```

**`services/mcp/requirements.txt`**

```txt
fastapi==0.112.0
uvicorn[standard]==0.30.5
gitpython==3.1.43
```

**`services/mcp/servers/fs_server.py`**

```python
from fastapi import FastAPI
from pydantic import BaseModel
import os, pathlib

ROOT = pathlib.Path("/workspace").resolve()
app = FastAPI(title="MCP FS Server")

class ReadBody(BaseModel):
    path: str

class WriteBody(BaseModel):
    path: str
    content: str

@app.get("/health")
def health():
    return {"ok": True, "root": str(ROOT)}

@app.post("/fs/read")
def fs_read(body: ReadBody):
    p = (ROOT / body.path.lstrip("/ ")).resolve()
    if not str(p).startswith(str(ROOT)):
        return {"error": "out-of-root"}
    if not p.exists():
        return {"error": "not-found"}
    return {"path": str(p), "content": p.read_text(encoding="utf-8", errors="ignore")[:200000]}

@app.post("/fs/write")
def fs_write(body: WriteBody):
    p = (ROOT / body.path.lstrip("/ ")).resolve()
    if not str(p).startswith(str(ROOT)):
        return {"error": "out-of-root"}
    os.makedirs(p.parent, exist_ok=True)
    p.write_text(body.content, encoding="utf-8")
    return {"ok": True, "path": str(p)}
```

> `git_server.py`, `shell_server.py`ë„ ê°™ì€ íŒ¨í„´ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥.

---

## âœ… 12) ì‚¬ìš© ìˆœì„œ ìš”ì•½ (Claude Code / Codex ì¹œí™”)

1. **ë¦¬í¬ ìƒì„±** í›„ ë³¸ ë¬¸ì„œëŒ€ë¡œ íŒŒì¼/í´ë” ìƒì„± â†’ `.env.example` ë³µì‚¬í•´ `.env` ì¤€ë¹„
2. **Phase 1**: `make up-p1` â†’ VS Codeì—ì„œ `http://localhost:8000/v1` ì„¤ì •
3. **Phase 2**: `make up-p2` â†’ `data/rag_docs`ì— ë¬¸ì„œ ë„£ê³  ì¸ë±ì‹±/ì§ˆì˜
4. **Phase 3**: `make up-p3` â†’ MCP ì—”ë“œí¬ì¸íŠ¸ë¡œ íŒŒì¼ ì½ê¸°/ì“°ê¸° ìë™í™” ì—°ê²°
5. ëª¨ë¸ ì „í™˜ì€ `.env`ì˜ `CHAT_MODEL`/`CODE_MODEL` êµì²´(íŒŒì¼ë§Œ ë°”ê¾¸ë©´ ì¦‰ì‹œ ë°˜ì˜)

---

## ğŸ§ª 13) í—¬ìŠ¤ì²´í¬ ì»¤ë§¨ë“œ ëª¨ìŒ

```bash
# ê²Œì´íŠ¸ì›¨ì´(Phase 1~)
curl http://localhost:8000/v1/models

# ì¶”ë¡  ì„œë²„(Phase 1~)
curl http://localhost:8001/health || true  # ì¼ë¶€ ë¹Œë“œì—” /health ë¯¸ì œê³µ

# Qdrant ëŒ€ì‹œë³´ë“œ(Phase 2~)
open http://localhost:6333/dashboard || xdg-open http://localhost:6333/dashboard || start http://localhost:6333/dashboard

# RAG(Phase 2~)
curl -X POST "http://localhost:8002/index?collection=test"
curl -H "Content-Type: application/json" -d '{"query":"hello","collection":"test"}' http://localhost:8002/query

# MCP(Phase 3~)
curl http://localhost:8020/health
```

---

## ğŸ“ ë©”ëª¨

* ì„±ëŠ¥ì´ ëŠë¦¬ë©´ `--parallel`, `--ctx-size`, ì–‘ìí™”(q4\_k\_m) ì¡°ì •
* ê¸´ ë¬¸ì„œ ì§ˆì˜ëŠ” RAGë¥¼ ê¸°ë³¸ ê²½ë¡œë¡œ(ì»¨í…ìŠ¤íŠ¸ ìµœì†Œí™”)
* ë¡œì»¬ ì „ì œë¼ ì¸ì¦ ìƒëµ. ì™¸ë¶€ ë…¸ì¶œ ê¸ˆì§€

```
```
