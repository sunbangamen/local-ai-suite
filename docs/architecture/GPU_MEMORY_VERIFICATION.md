# GPU Memory Verification for Dual Inference Setup

**ì‘ì„±ì¼**: 2025-10-09
**ëŒ€ìƒ GPU**: NVIDIA RTX 4050 (6GB VRAM)
**ëª©ì **: Phase 2 ì´ì¤‘í™” êµ¬ì„± ì‹œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²€ì¦

---

## ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„

### Scenario 1: 7B + 7B (ê¸°ì¡´ ê³„íš)

| êµ¬ì„± ìš”ì†Œ | ëª¨ë¸ | VRAM ì˜ˆìƒ ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|----------|------|------------------|------|
| **inference-chat** | Qwen2.5-7B-Instruct-Q4_K_M | ~4.4GB | 7B íŒŒë¼ë¯¸í„°, Q4_K_M ì–‘ìí™” |
| **inference-code** | Qwen2.5-Coder-7B-Instruct-Q4_K_M | ~4.4GB | 7B íŒŒë¼ë¯¸í„°, Q4_K_M ì–‘ìí™” |
| **ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ** | - | ~0.5GB | CUDA context, í”„ë ˆì„ì›Œí¬ |
| **ì´ VRAM í•„ìš”ëŸ‰** | - | **9.3GB** | âŒ **ì´ˆê³¼** |

**ê²°ë¡ **: RTX 4050 6GBë¡œëŠ” **ë¶ˆê°€ëŠ¥**

---

### Scenario 2: 3B + 7B (ëŒ€ì•ˆ - ì±„íƒ)

| êµ¬ì„± ìš”ì†Œ | ëª¨ë¸ | VRAM ì˜ˆìƒ ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|----------|------|------------------|------|
| **inference-chat** | Qwen2.5-3B-Instruct-Q4_K_M | ~2.2GB | 3B íŒŒë¼ë¯¸í„°, Q4_K_M ì–‘ìí™” |
| **inference-code** | Qwen2.5-Coder-7B-Instruct-Q4_K_M | ~4.4GB | 7B íŒŒë¼ë¯¸í„°, Q4_K_M ì–‘ìí™” |
| **ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ** | - | ~0.5GB | CUDA context, í”„ë ˆì„ì›Œí¬ |
| **ì´ VRAM í•„ìš”ëŸ‰** | - | **7.1GB** | âš ï¸ **ê²½ê³„ì„ ** |

**ê²°ë¡ **:
- ì´ë¡ ìƒ ì´ˆê³¼í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” **ì¼ë¶€ ë ˆì´ì–´ CPU ì˜¤í”„ë¡œë“œ** ê°€ëŠ¥
- `--n-gpu-layers` ì¡°ì •ìœ¼ë¡œ VRAM ì‚¬ìš©ëŸ‰ ì œì–´
- **ì±„íƒ ê°€ëŠ¥** (ì„±ëŠ¥ ì €í•˜ ê°ìˆ˜)

---

### Scenario 3: 3B + 7B (CPU Fallback ìµœì í™”)

| êµ¬ì„± ìš”ì†Œ | ëª¨ë¸ | GPU ë ˆì´ì–´ | VRAM ì‚¬ìš©ëŸ‰ | CPU ë ˆì´ì–´ |
|----------|------|-----------|-------------|-----------|
| **inference-chat** | Qwen2.5-3B-Instruct-Q4_K_M | 999 (ì „ì²´) | ~2.2GB | 0 |
| **inference-code** | Qwen2.5-Coder-7B-Instruct-Q4_K_M | 20-25 | ~2.5GB | ë‚˜ë¨¸ì§€ |
| **ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ** | - | - | ~0.5GB | - |
| **ì´ VRAM ì‚¬ìš©ëŸ‰** | - | - | **~5.2GB** | âœ… **ì•ˆì „** |

**ì¥ì **:
- VRAM ì—¬ìœ  í™•ë³´ (~0.8GB ë²„í¼)
- ì•ˆì •ì ì¸ ë™ì‹œ ì‹¤í–‰
- OOM (Out of Memory) ìœ„í—˜ ìµœì†Œí™”

**ë‹¨ì **:
- Code ëª¨ë¸ ì¶”ë¡  ì†ë„ ì•½ 20-30% ì €í•˜
- CPU ë³‘ëª© ê°€ëŠ¥ì„±

**ê²°ë¡ **: **ê¶Œì¥ ì„¤ì •** (ì•ˆì •ì„± ìš°ì„ )

---

## ğŸ”§ Llama.cpp GPU ë ˆì´ì–´ ì œì–´

### í™˜ê²½ë³€ìˆ˜ ì¡°ì •

```bash
# compose.p2.yml ë˜ëŠ” .env íŒŒì¼

# Chat ëª¨ë¸ (3B): ì „ì²´ GPU ì‚¬ìš©
CHAT_MODEL_N_GPU_LAYERS=999

# Code ëª¨ë¸ (7B): ì¼ë¶€ CPU ì˜¤í”„ë¡œë“œ
CODE_MODEL_N_GPU_LAYERS=20
```

### Docker Compose ì ìš© ì˜ˆì‹œ

```yaml
services:
  inference-chat:
    command: >
      --host 0.0.0.0 --port 8001
      --model /models/${CHAT_MODEL:-Qwen2.5-3B-Instruct-Q4_K_M.gguf}
      --n-gpu-layers ${CHAT_N_GPU_LAYERS:-999}  # ì „ì²´ GPU
      -c 1024 -b 128 -t 4
      --parallel 1 --cont-batching

  inference-code:
    command: >
      --host 0.0.0.0 --port 8001
      --model /models/${CODE_MODEL:-qwen2.5-coder-7b-instruct-q4_k_m.gguf}
      --n-gpu-layers ${CODE_N_GPU_LAYERS:-20}   # ì¼ë¶€ CPU ì˜¤í”„ë¡œë“œ
      -c 1024 -b 128 -t 4
      --parallel 1 --cont-batching
```

---

## ğŸ§ª ì‹¤ì œ ê²€ì¦ ë°©ë²•

### 1. VRAM ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```bash
# nvidia-smië¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë˜ëŠ” ìƒì„¸ ì •ë³´ ì¶œë ¥
nvidia-smi --query-gpu=memory.used,memory.total --format=csv -l 1
```

### 2. ì»¨í…Œì´ë„ˆë³„ VRAM ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# inference-chat ì‹œì‘ í›„ VRAM ì²´í¬
docker compose -f docker/compose.p2.yml up -d inference-chat
nvidia-smi

# inference-code ì‹œì‘ í›„ VRAM ì²´í¬
docker compose -f docker/compose.p2.yml up -d inference-code
nvidia-smi
```

### 3. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# Chat ëª¨ë¸ ë™ì‹œ ìš”ì²­ (10ê°œ)
seq 1 10 | xargs -P 10 -I {} curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]}'

# Code ëª¨ë¸ ë™ì‹œ ìš”ì²­ (10ê°œ)
seq 1 10 | xargs -P 10 -I {} curl -X POST http://localhost:8004/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "íŒŒì´ì¬ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜"}]}'
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ë¹„êµ

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ | Chat ì†ë„ | Code ì†ë„ | ì•ˆì •ì„± | ê¶Œì¥ë„ |
|------|------------|----------|----------|--------|--------|
| **7B + 7B (ë¶ˆê°€ëŠ¥)** | 9.3GB | - | - | âŒ OOM | âŒ |
| **3B + 7B (ì „ì²´ GPU)** | 7.1GB | 100% | 100% | âš ï¸ ë¶ˆì•ˆì • | âš ï¸ |
| **3B + 7B (CPU ì˜¤í”„ë¡œë“œ)** | 5.2GB | 100% | 70-80% | âœ… ì•ˆì • | âœ… **ê¶Œì¥** |

---

## ğŸ¯ Phase 2 ìµœì¢… ê¶Œì¥ ì„¤ì •

### .env íŒŒì¼

```bash
# Phase 2: RTX 4050 6GB ìµœì í™”
CHAT_MODEL=Qwen2.5-3B-Instruct-Q4_K_M.gguf
CODE_MODEL=qwen2.5-coder-7b-instruct-q4_k_m.gguf

# GPU ë ˆì´ì–´ ì„¤ì •
CHAT_N_GPU_LAYERS=999   # Chat: ì „ì²´ GPU (2.2GB)
CODE_N_GPU_LAYERS=20    # Code: ì¼ë¶€ CPU ì˜¤í”„ë¡œë“œ (2.5GB)
```

### ë©”ëª¨ë¦¬ ì œí•œ

```yaml
services:
  inference-chat:
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 6G    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì œí•œ

  inference-code:
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 6G
```

---

## ğŸ” ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ë°œìƒ ì‹œ

1. **Code ëª¨ë¸ GPU ë ˆì´ì–´ ê°ì†Œ**
   ```bash
   CODE_N_GPU_LAYERS=15  # 20 â†’ 15ë¡œ ê°ì†Œ
   ```

2. **Context í¬ê¸° ê°ì†Œ**
   ```bash
   -c 512  # 1024 â†’ 512ë¡œ ê°ì†Œ
   ```

3. **ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”**
   ```bash
   --parallel 1  # ì´ë¯¸ ìµœì†Œê°’
   ```

### ì„±ëŠ¥ ì €í•˜ ë°œìƒ ì‹œ

1. **CPU ìŠ¤ë ˆë“œ ì¦ê°€**
   ```bash
   -t 6  # 4 â†’ 6ìœ¼ë¡œ ì¦ê°€
   ```

2. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**
   ```bash
   -b 256  # 128 â†’ 256ìœ¼ë¡œ ì¦ê°€
   ```

---

## ğŸ“Š ëª¨ë¸ íŒŒì¼ í¬ê¸° ì°¸ê³ 

| ëª¨ë¸ | íŒŒì¼ í¬ê¸° | VRAM ì˜ˆìƒ (ì „ì²´ GPU) |
|------|----------|---------------------|
| Qwen2.5-3B-Instruct-Q4_K_M | ~2.0GB | ~2.2GB |
| Qwen2.5-7B-Instruct-Q4_K_M | ~4.1GB | ~4.4GB |
| Qwen2.5-Coder-7B-Instruct-Q4_K_M | ~4.1GB | ~4.4GB |
| Qwen2.5-14B-Instruct-Q4_K_M | ~8.2GB | ~8.8GB (RTX 4050 ë¶ˆê°€) |

---

## âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

Phase 2 ë°°í¬ ì „ í™•ì¸ ì‚¬í•­:

- [ ] 3B Chat ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ (`Qwen2.5-3B-Instruct-Q4_K_M.gguf`)
- [ ] 7B Code ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ (`qwen2.5-coder-7b-instruct-q4_k_m.gguf`)
- [ ] `.env` íŒŒì¼ì— GPU ë ˆì´ì–´ ì„¤ì • ì ìš©
- [ ] `nvidia-smi`ë¡œ í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰ í™•ì¸ (< 1GB)
- [ ] inference-chat ë‹¨ë… ì‹œì‘ í›„ VRAM ì²´í¬ (~2.2GB)
- [ ] inference-code ì¶”ê°€ ì‹œì‘ í›„ VRAM ì²´í¬ (~4.7GB ì´í•˜)
- [ ] ì–‘ìª½ ëª¨ë¸ ë™ì‹œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] 30ë¶„ ë¶€í•˜ í…ŒìŠ¤íŠ¸ í›„ OOM ë¯¸ë°œìƒ í™•ì¸

---

**ê²€ì¦ ë‹´ë‹¹**: ì‹œìŠ¤í…œ ìš´ì˜ì
**ê²€ì¦ ì™„ë£Œ ì‹œ**: `docs/progress/v1/fb_7.md`ì— ê²°ê³¼ ê¸°ë¡
