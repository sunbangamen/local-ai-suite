#!/usr/bin/env python3
"""
Memory System Performance Benchmark
ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê²€ì¦ - 100ë§Œê°œ ëŒ€í™” ì €ì¥/ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬
"""

import sys
import time
import asyncio
from pathlib import Path
from datetime import datetime
import random
import json

# Add project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from memory_system import MemorySystem


class MemoryBenchmark:
    def __init__(self, test_size: int = 1000):
        """
        Args:
            test_size: í…ŒìŠ¤íŠ¸í•  ëŒ€í™” ê°œìˆ˜ (ê¸°ë³¸ 1000, ì‹¤ì œ 100ë§Œ í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ ì†Œìš”)
        """
        self.test_size = test_size
        self.memory = MemorySystem(data_dir="/tmp/benchmark-memory")
        self.project_id = None
        self.results = {}

    def setup(self):
        """ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì •"""
        print("=" * 80)
        print(f"Memory System Performance Benchmark - {self.test_size:,} conversations")
        print("=" * 80)

        self.project_id = self.memory.get_project_id("/tmp/benchmark-project")
        print(f"âœ… Benchmark project: {self.project_id}\n")

    def generate_test_conversation(self, index: int) -> tuple:
        """í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ìƒì„±"""
        queries = [
            f"Pythonì—ì„œ íŒŒì¼ ì½ëŠ” ë°©ë²• {index}",
            f"Django REST API ì„¤ê³„ íŒ¨í„´ {index}",
            f"React Hook ì‚¬ìš©ë²• {index}",
            f"SQL ì¿¼ë¦¬ ìµœì í™” ë°©ë²• {index}",
            f"Docker ì»¨í…Œì´ë„ˆ ê´€ë¦¬ {index}",
        ]

        responses = [
            f"Pythonì—ì„œëŠ” open() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì œ: with open('file.txt', 'r') as f: data = f.read() #{index}",
            f"Django REST Frameworkì—ì„œëŠ” ViewSetê³¼ Serializerë¥¼ ì‚¬ìš©í•˜ì—¬ APIë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. #{index}",
            f"React Hookì€ useStateì™€ useEffectë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆì œ: const [state, setState] = useState(null) #{index}",
            f"SQL ì¿¼ë¦¬ ìµœì í™”ëŠ” ì¸ë±ìŠ¤ ì¶”ê°€, ì¿¼ë¦¬ êµ¬ì¡° ê°œì„ , EXPLAIN ë¶„ì„ ë“±ì„ í†µí•´ ê°€ëŠ¥í•©ë‹ˆë‹¤. #{index}",
            f"Docker ì»¨í…Œì´ë„ˆëŠ” docker ps, docker logs, docker exec ë“±ì˜ ëª…ë ¹ì–´ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. #{index}",
        ]

        models = ["chat-7b", "code-7b"]

        query = random.choice(queries)
        response = random.choice(responses)
        model = random.choice(models)

        return query, response, model

    def benchmark_save(self):
        """ëŒ€í™” ì €ì¥ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"[Benchmark 1] ëŒ€í™” ì €ì¥ ì„±ëŠ¥ ({self.test_size:,}ê°œ)")
        print("-" * 80)

        conversation_ids = []
        start_time = time.time()

        for i in range(self.test_size):
            query, response, model = self.generate_test_conversation(i)

            conv_id = self.memory.save_conversation(
                project_id=self.project_id,
                user_query=query,
                ai_response=response,
                model_used=model,
                session_id=f"benchmark-session-{i // 100}",
            )

            if conv_id:
                conversation_ids.append(conv_id)

            # Progress indicator
            if (i + 1) % 100 == 0:
                elapsed = time.time() - start_time
                rate = (i + 1) / elapsed
                print(
                    f"  ì§„í–‰: {i + 1:,}/{self.test_size:,} ({rate:.1f} conversations/sec)",
                    end="\r",
                )

        elapsed_time = time.time() - start_time
        avg_time_per_conv = (elapsed_time / self.test_size) * 1000  # ms

        print(f"\nâœ… ì €ì¥ ì™„ë£Œ:")
        print(f"   ì´ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì €ì¥ ì‹œê°„: {avg_time_per_conv:.2f}ms/conversation")
        print(f"   ì²˜ë¦¬ëŸ‰: {self.test_size / elapsed_time:.1f} conversations/sec")

        self.results["save"] = {
            "total_conversations": len(conversation_ids),
            "elapsed_time_sec": elapsed_time,
            "avg_time_ms": avg_time_per_conv,
            "throughput_per_sec": self.test_size / elapsed_time,
        }

        return conversation_ids

    def benchmark_fts_search(self, num_queries: int = 100):
        """FTS5 ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\n[Benchmark 2] FTS5 ê²€ìƒ‰ ì„±ëŠ¥ ({num_queries}ê°œ ì¿¼ë¦¬)")
        print("-" * 80)

        test_queries = [
            "Python íŒŒì¼",
            "Django API",
            "React Hook",
            "SQL ì¿¼ë¦¬",
            "Docker ì»¨í…Œì´ë„ˆ",
        ]

        search_times = []
        total_results = 0

        for i in range(num_queries):
            query = random.choice(test_queries)

            start_time = time.time()
            results = self.memory.search_conversations(
                project_id=self.project_id, query=query, limit=10
            )
            elapsed_ms = (time.time() - start_time) * 1000

            search_times.append(elapsed_ms)
            total_results += len(results)

            if (i + 1) % 20 == 0:
                print(f"  ì§„í–‰: {i + 1}/{num_queries}", end="\r")

        avg_time = sum(search_times) / len(search_times)
        p95_time = sorted(search_times)[int(len(search_times) * 0.95)]
        p99_time = sorted(search_times)[int(len(search_times) * 0.99)]

        print(f"\nâœ… ê²€ìƒ‰ ì™„ë£Œ:")
        print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.2f}ms")
        print(f"   P95 ê²€ìƒ‰ ì‹œê°„: {p95_time:.2f}ms")
        print(f"   P99 ê²€ìƒ‰ ì‹œê°„: {p99_time:.2f}ms")
        print(f"   í‰ê·  ê²°ê³¼ ìˆ˜: {total_results / num_queries:.1f}ê°œ")
        print(
            f"   ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {'âœ… PASS' if p95_time < 1000 else 'âŒ FAIL'} (ëª©í‘œ: < 1000ms)"
        )

        self.results["fts_search"] = {
            "num_queries": num_queries,
            "avg_time_ms": avg_time,
            "p95_time_ms": p95_time,
            "p99_time_ms": p99_time,
            "avg_results": total_results / num_queries,
            "target_met": p95_time < 1000,
        }

    async def benchmark_vector_search(self, num_queries: int = 50):
        """ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\n[Benchmark 3] ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ({num_queries}ê°œ ì¿¼ë¦¬)")
        print("-" * 80)

        if not self.memory._vector_enabled:
            print("âš ï¸ ë²¡í„° ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            self.results["vector_search"] = {"skipped": True}
            return

        # ë¨¼ì € ì„ë² ë”© ìƒì„±
        print("  ì„ë² ë”© ìƒì„± ì¤‘...")
        start_embed = time.time()
        processed = await self.memory.process_pending_embeddings(
            self.project_id, batch_size=64
        )
        embed_time = time.time() - start_embed
        print(f"  âœ… {processed}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ ({embed_time:.2f}ì´ˆ)")

        # ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = [
            "íŒŒì¼ ì…ì¶œë ¥ ì²˜ë¦¬ ë°©ë²•",
            "ì›¹ API ì„¤ê³„ ë° êµ¬í˜„",
            "í”„ë¡ íŠ¸ì—”ë“œ ìƒíƒœ ê´€ë¦¬",
            "ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”",
            "ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
        ]

        search_times = []
        total_results = 0

        for i in range(num_queries):
            query = random.choice(test_queries)

            start_time = time.time()
            results = await self.memory.vector_search_conversations(
                project_id=self.project_id, query=query, limit=10, score_threshold=0.5
            )
            elapsed_ms = (time.time() - start_time) * 1000

            search_times.append(elapsed_ms)
            total_results += len(results)

            if (i + 1) % 10 == 0:
                print(f"  ì§„í–‰: {i + 1}/{num_queries}", end="\r")

        avg_time = sum(search_times) / len(search_times)
        p95_time = sorted(search_times)[int(len(search_times) * 0.95)]

        print(f"\nâœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ:")
        print(f"   ì„ë² ë”© ìƒì„±: {embed_time:.2f}ì´ˆ ({processed}ê°œ)")
        print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.2f}ms")
        print(f"   P95 ê²€ìƒ‰ ì‹œê°„: {p95_time:.2f}ms")
        print(f"   í‰ê·  ê²°ê³¼ ìˆ˜: {total_results / num_queries:.1f}ê°œ")

        self.results["vector_search"] = {
            "num_queries": num_queries,
            "embeddings_generated": processed,
            "embedding_time_sec": embed_time,
            "avg_search_time_ms": avg_time,
            "p95_search_time_ms": p95_time,
            "avg_results": total_results / num_queries,
        }

    async def benchmark_hybrid_search(self, num_queries: int = 50):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\n[Benchmark 4] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ({num_queries}ê°œ ì¿¼ë¦¬)")
        print("-" * 80)

        if not self.memory._vector_enabled:
            print("âš ï¸ ë²¡í„° ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            self.results["hybrid_search"] = {"skipped": True}
            return

        test_queries = [
            "Python í”„ë¡œê·¸ë˜ë°",
            "ì›¹ ê°œë°œ",
            "ë°ì´í„°ë² ì´ìŠ¤",
            "í´ë¼ìš°ë“œ ì¸í”„ë¼",
            "DevOps ìë™í™”",
        ]

        search_times = []
        total_results = 0

        for i in range(num_queries):
            query = random.choice(test_queries)

            start_time = time.time()
            results = await self.memory.hybrid_search_conversations(
                project_id=self.project_id, query=query, limit=10
            )
            elapsed_ms = (time.time() - start_time) * 1000

            search_times.append(elapsed_ms)
            total_results += len(results)

            if (i + 1) % 10 == 0:
                print(f"  ì§„í–‰: {i + 1}/{num_queries}", end="\r")

        avg_time = sum(search_times) / len(search_times)
        p95_time = sorted(search_times)[int(len(search_times) * 0.95)]

        print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ:")
        print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.2f}ms")
        print(f"   P95 ê²€ìƒ‰ ì‹œê°„: {p95_time:.2f}ms")
        print(f"   í‰ê·  ê²°ê³¼ ìˆ˜: {total_results / num_queries:.1f}ê°œ")

        self.results["hybrid_search"] = {
            "num_queries": num_queries,
            "avg_time_ms": avg_time,
            "p95_time_ms": p95_time,
            "avg_results": total_results / num_queries,
        }

    def benchmark_stats(self):
        """í†µê³„ ì¡°íšŒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\n[Benchmark 5] í†µê³„ ì¡°íšŒ ì„±ëŠ¥")
        print("-" * 80)

        start_time = time.time()
        stats = self.memory.get_conversation_stats(self.project_id)
        elapsed_ms = (time.time() - start_time) * 1000

        print(f"âœ… í†µê³„ ì¡°íšŒ ì™„ë£Œ: {elapsed_ms:.2f}ms")
        print(f"   ì´ ëŒ€í™”: {stats['total_conversations']:,}ê°œ")
        print(f"   í‰ê·  ì¤‘ìš”ë„: {stats['avg_importance']:.2f}/10")

        self.results["stats"] = {
            "query_time_ms": elapsed_ms,
            "total_conversations": stats["total_conversations"],
        }

    def save_results(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        output_file = (
            Path("/tmp")
            / f"memory_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {output_file}")

    def print_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("Benchmark Summary")
        print("=" * 80)

        # ì €ì¥ ì„±ëŠ¥
        if "save" in self.results:
            save = self.results["save"]
            print(f"\nğŸ“ ì €ì¥ ì„±ëŠ¥:")
            print(f"   ì²˜ë¦¬ëŸ‰: {save['throughput_per_sec']:.1f} conversations/sec")
            print(f"   í‰ê·  ì‹œê°„: {save['avg_time_ms']:.2f}ms")
            print(
                f"   ëª©í‘œ ë‹¬ì„±: {'âœ… PASS' if save['avg_time_ms'] < 100 else 'âš ï¸  SLOW'} (ëª©í‘œ: < 100ms)"
            )

        # FTS5 ê²€ìƒ‰
        if "fts_search" in self.results:
            fts = self.results["fts_search"]
            print(f"\nğŸ” FTS5 ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"   í‰ê· : {fts['avg_time_ms']:.2f}ms")
            print(f"   P95: {fts['p95_time_ms']:.2f}ms")
            print(
                f"   ëª©í‘œ ë‹¬ì„±: {'âœ… PASS' if fts['target_met'] else 'âŒ FAIL'} (ëª©í‘œ: < 1000ms)"
            )

        # ë²¡í„° ê²€ìƒ‰
        if "vector_search" in self.results and not self.results["vector_search"].get(
            "skipped"
        ):
            vec = self.results["vector_search"]
            print(f"\nğŸ§  ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"   í‰ê· : {vec['avg_search_time_ms']:.2f}ms")
            print(f"   P95: {vec['p95_search_time_ms']:.2f}ms")

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        if "hybrid_search" in self.results and not self.results["hybrid_search"].get(
            "skipped"
        ):
            hyb = self.results["hybrid_search"]
            print(f"\nğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"   í‰ê· : {hyb['avg_time_ms']:.2f}ms")
            print(f"   P95: {hyb['p95_time_ms']:.2f}ms")

        print("\n" + "=" * 80)


async def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="Memory System Performance Benchmark")
    parser.add_argument(
        "--size",
        type=int,
        default=1000,
        help="Number of conversations to test (default: 1000)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Run full 1M conversation test (takes ~1 hour)",
    )
    args = parser.parse_args()

    test_size = 1_000_000 if args.full else args.size

    if args.full:
        print("âš ï¸  Full 1M conversation test will take approximately 1 hour")
        response = input("Continue? (yes/no): ")
        if response.lower() != "yes":
            print("Cancelled.")
            return 1

    benchmark = MemoryBenchmark(test_size=test_size)

    try:
        # Setup
        benchmark.setup()

        # Run benchmarks
        benchmark.benchmark_save()
        benchmark.benchmark_fts_search(num_queries=100)
        await benchmark.benchmark_vector_search(num_queries=50)
        await benchmark.benchmark_hybrid_search(num_queries=50)
        benchmark.benchmark_stats()

        # Results
        benchmark.save_results()
        benchmark.print_summary()

        print("\nâœ… ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        return 0

    except Exception as e:
        print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
