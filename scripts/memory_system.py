"""
AI Memory System - í”„ë¡œì íŠ¸ë³„ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ
SQLite ê¸°ë°˜ ëŒ€í™” ì €ì¥, ê²€ìƒ‰, ìë™ ì •ë¦¬ ê¸°ëŠ¥ ì œê³µ
"""

import sqlite3
import json
import hashlib
import uuid
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from contextlib import contextmanager
import threading
import re
import os
# Vector search dependencies (optional)
try:
    import httpx
    from qdrant_client import QdrantClient
    from qdrant_client.http import models as qmodels
    VECTOR_DEPS_AVAILABLE = True
except ImportError:
    VECTOR_DEPS_AVAILABLE = False
    # Mock classes for type hints when dependencies are not available
    class QdrantClient:
        pass
    class qmodels:
        class VectorParams:
            pass
        class Distance:
            COSINE = None
        class PointStruct:
            pass

class MemorySystem:
    def __init__(self, data_dir: str = None):
        self.data_dir = self._get_data_directory(data_dir)
        self._local = threading.local()
        self._storage_available = True

        # ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰ ì„¤ì •
        self.embedding_url = os.getenv("EMBEDDING_URL", "http://localhost:8003")
        self.qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
        self._qdrant_client = None
        self._embedding_dim = None
        self._vector_enabled = VECTOR_DEPS_AVAILABLE

        if not VECTOR_DEPS_AVAILABLE:
            print("ğŸ’¡ Vector search dependencies not available. Text search only.")

        # í”„ë¡œì íŠ¸ë³„ ë©”ëª¨ë¦¬ ë””ë ‰í† ë¦¬
        try:
            self.projects_dir = self.data_dir / "projects"
            self.projects_dir.mkdir(parents=True, exist_ok=True)

            # ê¸€ë¡œë²Œ ì„¤ì • ë””ë ‰í† ë¦¬
            self.global_dir = self.data_dir / "global"
            self.global_dir.mkdir(exist_ok=True)
        except (OSError, PermissionError) as e:
            print(f"âš ï¸ Warning: Cannot create memory directories: {e}")
            print(f"ğŸ’¡ Memory system will be disabled for this session.")
            self._storage_available = False

        # ì¤‘ìš”ë„ ë ˆë²¨ ì •ì˜
        self.IMPORTANCE_LEVELS = {
            1: {"name": "ì¦‰ì‹œì‚­ì œ", "ttl_days": 0, "description": "ì¸ì‚¬, í…ŒìŠ¤íŠ¸"},
            2: {"name": "ë‹¨ê¸°ë³´ê´€", "ttl_days": 3, "description": "ê°„ë‹¨í•œ ì§ˆë¬¸"},
            3: {"name": "1ì£¼ë³´ê´€", "ttl_days": 7, "description": "ì¼ë°˜ ëŒ€í™”"},
            4: {"name": "2ì£¼ë³´ê´€", "ttl_days": 14, "description": "ì •ë³´ì„± ì§ˆë¬¸"},
            5: {"name": "ê¸°ë³¸ë³´ê´€", "ttl_days": 30, "description": "ê¸°ë³¸ê°’"},
            6: {"name": "1ê°œì›”", "ttl_days": 30, "description": "ì½”ë“œ ê´€ë ¨"},
            7: {"name": "3ê°œì›”", "ttl_days": 90, "description": "í”„ë¡œì íŠ¸ ì„¤ì •"},
            8: {"name": "6ê°œì›”", "ttl_days": 180, "description": "ì¤‘ìš” ê²°ì •ì‚¬í•­"},
            9: {"name": "1ë…„ë³´ê´€", "ttl_days": 365, "description": "í•µì‹¬ ë¬¸ì„œí™”"},
            10: {"name": "ì˜êµ¬ë³´ê´€", "ttl_days": -1, "description": "ì‚¬ìš©ì ì¤‘ìš”í‘œì‹œ"}
        }

    def _get_data_directory(self, data_dir: str = None) -> Path:
        """
        ë©”ëª¨ë¦¬ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ê²°ì •
        ìš°ì„ ìˆœìœ„: 1. ëª…ì‹œëœ ê²½ë¡œ 2. í™˜ê²½ë³€ìˆ˜ 3. ê¸°ë³¸ ê²½ë¡œ 4. í”„ë¡œì íŠ¸ ë¡œì»¬ í´ë°±
        """
        # 1. ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ ê²½ë¡œ
        if data_dir:
            path = Path(data_dir)
            if self._test_directory_access(path):
                return path

        # 2. í™˜ê²½ë³€ìˆ˜ AI_MEMORY_DIR
        env_dir = os.environ.get('AI_MEMORY_DIR')
        if env_dir:
            path = Path(env_dir)
            if self._test_directory_access(path):
                return path
            else:
                print(f"âš ï¸ Warning: AI_MEMORY_DIR '{env_dir}' is not accessible")

        # 3. ê¸°ë³¸ ê²½ë¡œ ì‹œë„
        default_path = Path("/mnt/e/ai-data/memory")
        if self._test_directory_access(default_path):
            return default_path

        # 4. í”„ë¡œì íŠ¸ ë¡œì»¬ í´ë°±
        current_repo = self._find_git_root()
        if current_repo:
            fallback_path = current_repo / ".ai-memory-data"
            print(f"ğŸ’¡ Using local memory storage: {fallback_path}")
            if self._test_directory_access(fallback_path):
                return fallback_path

        # 5. ìµœì¢… í´ë°± - í˜„ì¬ ë””ë ‰í† ë¦¬
        final_fallback = Path.cwd() / ".ai-memory-data"
        print(f"âš ï¸ Using current directory fallback: {final_fallback}")
        return final_fallback

    def _test_directory_access(self, path: Path) -> bool:
        """ë””ë ‰í† ë¦¬ ì ‘ê·¼ ê¶Œí•œ í…ŒìŠ¤íŠ¸"""
        try:
            path.mkdir(parents=True, exist_ok=True)
            # ì“°ê¸° ê¶Œí•œ í…ŒìŠ¤íŠ¸
            test_file = path / ".test_write"
            test_file.write_text("test")
            test_file.unlink()
            return True
        except (OSError, PermissionError):
            return False

    def _find_git_root(self) -> Optional[Path]:
        """í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ Git ë£¨íŠ¸ ì°¾ê¸°"""
        current = Path.cwd()
        while current != current.parent:
            if (current / ".git").exists():
                return current
            current = current.parent
        return None

    def get_project_id(self, project_path: str = None) -> str:
        """
        í”„ë¡œì íŠ¸ ID íšë“ ë˜ëŠ” ìƒì„±
        .ai-memory/project.jsonì—ì„œ UUID ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
        Docker í™˜ê²½ì—ì„œëŠ” ì¤‘ì•™ì§‘ì¤‘ì‹ ë©”ëª¨ë¦¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        """
        # Docker í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
        default_project_id = os.getenv('DEFAULT_PROJECT_ID')
        if default_project_id:
            return default_project_id

        if project_path is None:
            project_path = os.getcwd()

        project_path = Path(project_path).resolve()

        # Docker í™˜ê²½ ê°ì§€ (/.dockerenv íŒŒì¼ ì¡´ì¬ ì—¬ë¶€)
        if Path("/.dockerenv").exists():
            # Docker í™˜ê²½ì—ì„œëŠ” ì¤‘ì•™ì§‘ì¤‘ì‹ ì €ì¥ì†Œ ì‚¬ìš©
            # ì‹¤ì œ ê²½ë¡œ: /app/memory/projects/docker-default
            memory_dir = self.projects_dir / "docker-default"
            project_file = memory_dir / "project.json"
        else:
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í”„ë¡œì íŠ¸ë³„ .ai-memory ë””ë ‰í† ë¦¬ ì‚¬ìš©
            memory_dir = project_path / ".ai-memory"
            project_file = memory_dir / "project.json"

        # ê¸°ì¡´ í”„ë¡œì íŠ¸ íŒŒì¼ í™•ì¸
        if project_file.exists():
            try:
                with open(project_file, 'r', encoding='utf-8') as f:
                    project_data = json.load(f)
                    return project_data['project_id']
            except (json.JSONDecodeError, KeyError):
                pass

        # ìƒˆ í”„ë¡œì íŠ¸ ID ìƒì„±
        project_id = str(uuid.uuid4())
        memory_dir.mkdir(exist_ok=True)

        project_data = {
            "project_id": project_id,
            "project_name": project_path.name,
            "project_path": str(project_path),
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

        with open(project_file, 'w', encoding='utf-8') as f:
            json.dump(project_data, f, indent=2, ensure_ascii=False)

        return project_id

    def get_project_db_path(self, project_id: str) -> Path:
        """í”„ë¡œì íŠ¸ë³„ SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
        project_dir = self.projects_dir / project_id
        project_dir.mkdir(exist_ok=True)
        return project_dir / "memory.db"

    def _get_connection(self, project_id: str):
        """Thread-safe í”„ë¡œì íŠ¸ë³„ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        if not hasattr(self._local, 'connections'):
            self._local.connections = {}

        if project_id not in self._local.connections:
            db_path = self.get_project_db_path(project_id)
            conn = sqlite3.connect(
                str(db_path),
                check_same_thread=False,
                timeout=30.0
            )
            conn.row_factory = sqlite3.Row

            # SQLite ì„±ëŠ¥ ìµœì í™”
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA foreign_keys=ON")

            self._local.connections[project_id] = conn

            # ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
            self._init_schema(conn)

        return self._local.connections[project_id]

    @contextmanager
    def transaction(self, project_id: str):
        """íŠ¸ëœì­ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = self._get_connection(project_id)
        try:
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise

    def _init_schema(self, conn):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""

        # ëŒ€í™” ê¸°ë¡ í…Œì´ë¸”
        conn.execute("""
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                user_query TEXT NOT NULL,
                ai_response TEXT NOT NULL,
                model_used VARCHAR(50),
                importance_score INTEGER DEFAULT 5,
                tags TEXT,  -- JSON ë°°ì—´
                session_id VARCHAR(50),
                token_count INTEGER,
                response_time_ms INTEGER,
                project_context TEXT,  -- í”„ë¡œì íŠ¸ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                expires_at DATETIME  -- TTL ê¸°ë°˜ ìë™ ì‚­ì œìš©
            )
        """)

        # ëŒ€í™” ìš”ì•½ í…Œì´ë¸”
        conn.execute("""
            CREATE TABLE IF NOT EXISTS conversation_summaries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date_range TEXT,  -- "2024-09-01 to 2024-09-07"
                summary TEXT,     -- AI ìƒì„± ìš”ì•½
                conversation_count INTEGER,
                importance_level INTEGER,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # ì¤‘ìš” ì‚¬ì‹¤ í…Œì´ë¸”
        conn.execute("""
            CREATE TABLE IF NOT EXISTS important_facts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                fact TEXT NOT NULL,
                category VARCHAR(100),  -- code, config, decision, etc.
                source_conversation_id INTEGER,
                user_marked BOOLEAN DEFAULT FALSE,
                ai_suggested BOOLEAN DEFAULT FALSE,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_conversation_id) REFERENCES conversations(id)
            )
        """)

        # ì‚¬ìš©ì ì„ í˜¸ë„ í…Œì´ë¸”
        conn.execute("""
            CREATE TABLE IF NOT EXISTS user_preferences (
                key VARCHAR(100) PRIMARY KEY,
                value TEXT,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # ë²¡í„° ì„ë² ë”© í…Œì´ë¸” (Qdrant ë™ê¸°í™”ìš©)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS conversation_embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id INTEGER NOT NULL,
                embedding_vector TEXT,  -- JSON í˜•íƒœ ì„ë² ë”© ë²¡í„° (ë¡œì»¬ í´ë°±ìš©)
                qdrant_point_id TEXT,   -- Qdrant í¬ì¸íŠ¸ ID
                sync_status TEXT DEFAULT 'pending',  -- 'pending', 'synced', 'failed'
                synced_at DATETIME,     -- ë™ê¸°í™” ì™„ë£Œ ì‹œê°
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (conversation_id) REFERENCES conversations(id),
                UNIQUE(conversation_id)  -- í•œ ëŒ€í™”ë‹¹ í•˜ë‚˜ì˜ ì„ë² ë”©
            )
        """)

        # FTS5 ì „ë¬¸ ê²€ìƒ‰ í…Œì´ë¸”
        conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS conversations_fts USING fts5(
                user_query,
                ai_response,
                content='conversations',
                content_rowid='id'
            )
        """)

        # ì¸ë±ìŠ¤ ìƒì„±
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_conversations_timestamp ON conversations(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_importance ON conversations(importance_score)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_expires_at ON conversations(expires_at)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_session_id ON conversations(session_id)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_model_used ON conversations(model_used)",
            "CREATE INDEX IF NOT EXISTS idx_important_facts_category ON important_facts(category)",
            "CREATE INDEX IF NOT EXISTS idx_conversation_embeddings_sync_status ON conversation_embeddings(sync_status)"
        ]

        for index_sql in indexes:
            conn.execute(index_sql)

        # FTS5 íŠ¸ë¦¬ê±° (ìë™ ë™ê¸°í™”)
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS conversations_ai_insert AFTER INSERT ON conversations
            BEGIN
                INSERT INTO conversations_fts(rowid, user_query, ai_response)
                VALUES (NEW.id, NEW.user_query, NEW.ai_response);
            END
        """)

        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS conversations_ai_update AFTER UPDATE ON conversations
            BEGIN
                UPDATE conversations_fts SET
                    user_query = NEW.user_query,
                    ai_response = NEW.ai_response
                WHERE rowid = NEW.id;
            END
        """)

        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS conversations_ai_delete AFTER DELETE ON conversations
            BEGIN
                DELETE FROM conversations_fts WHERE rowid = OLD.id;
            END
        """)

    def calculate_importance_score(self, user_query: str, ai_response: str,
                                 model_used: str = None, context: Dict = None) -> int:
        """
        ëŒ€í™”ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ìë™ ê³„ì‚° (1-10)
        """
        score = 5  # ê¸°ë³¸ê°’
        context = context or {}

        query_lower = user_query.lower()
        response_lower = ai_response.lower()
        combined_text = f"{query_lower} {response_lower}"

        # ë†’ì€ ì¤‘ìš”ë„ í‚¤ì›Œë“œ
        high_importance_keywords = [
            # ê¸°ìˆ  ì„¤ì •
            "ì„¤ì •", "config", "configuration", "í™˜ê²½ë³€ìˆ˜", "environment",
            "architecture", "design pattern", "ì•„í‚¤í…ì²˜", "ì„¤ê³„",

            # ë¬¸ì œ í•´ê²°
            "ë²„ê·¸", "ì—ëŸ¬", "ì˜¤ë¥˜", "ë¬¸ì œ", "í•´ê²°", "fix", "bug", "error",
            "issue", "problem", "solution", "trouble",

            # ì¤‘ìš” ê°œë°œ
            "êµ¬í˜„", "implementation", "ì•Œê³ ë¦¬ì¦˜", "algorithm", "ìµœì í™”",
            "optimization", "performance", "ì„±ëŠ¥", "ë³´ì•ˆ", "security",

            # ê²°ì •ì‚¬í•­
            "ê²°ì •", "decision", "ì •ì±…", "policy", "ë°©í–¥", "direction",
            "ì „ëµ", "strategy", "ê³„íš", "plan"
        ]

        # ë‚®ì€ ì¤‘ìš”ë„ í‚¤ì›Œë“œ
        low_importance_keywords = [
            "ì•ˆë…•", "hello", "hi", "í…ŒìŠ¤íŠ¸", "test", "í™•ì¸", "check",
            "ê°ì‚¬", "thank", "ì¢‹ì•„", "ì¢‹ë‹¤", "ê´œì°®", "ok", "okay"
        ]

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        high_count = sum(1 for keyword in high_importance_keywords if keyword in combined_text)
        low_count = sum(1 for keyword in low_importance_keywords if keyword in combined_text)

        score += min(high_count, 3)  # ìµœëŒ€ +3
        score -= min(low_count, 2)   # ìµœëŒ€ -2

        # ì‘ë‹µ ê¸¸ì´ ê³ ë ¤ (ê¸´ ì‘ë‹µ = ë” ìƒì„¸í•œ ì •ë³´)
        response_length = len(ai_response)
        if response_length > 2000:
            score += 2
        elif response_length > 1000:
            score += 1
        elif response_length < 100:
            score -= 1

        # ì½”ë“œ í¬í•¨ ì—¬ë¶€
        code_patterns = [
            r'```[\s\S]*?```',  # ì½”ë“œ ë¸”ë¡
            r'`[^`]+`',         # ì¸ë¼ì¸ ì½”ë“œ
            r'def\s+\w+',       # Python í•¨ìˆ˜
            r'function\s+\w+',  # JavaScript í•¨ìˆ˜
            r'class\s+\w+',     # í´ë˜ìŠ¤ ì •ì˜
            r'import\s+\w+',    # Import ë¬¸
            r'SELECT\s+.*FROM', # SQL ì¿¼ë¦¬
        ]

        for pattern in code_patterns:
            if re.search(pattern, ai_response, re.IGNORECASE):
                score += 1
                break

        # ëª¨ë¸ íƒ€ì… ê³ ë ¤
        if model_used == "code-7b":
            score += 1  # ì½”ë”© ëª¨ë¸ ì‚¬ìš©ì‹œ +1

        # ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜
        if context.get("user_saved", False):
            score = 10  # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥í•œ ê²½ìš°
        elif context.get("user_important", False):
            score = max(score, 8)

        # ì§ˆë¬¸ ê¸¸ì´ ê³ ë ¤ (ìƒì„¸í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì¤‘ìš”)
        if len(user_query) > 200:
            score += 1

        return max(1, min(10, score))

    def save_conversation(self, project_id: str, user_query: str, ai_response: str,
                         model_used: str = None, session_id: str = None,
                         token_count: int = None, response_time_ms: int = None,
                         context: Dict = None, tags: List[str] = None) -> Optional[int]:
        """
        ëŒ€í™”ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        ê¶Œí•œ ì˜¤ë¥˜ ì‹œ None ë°˜í™˜
        """
        if not self._storage_available:
            return None

        try:
            context = context or {}
            importance_score = self.calculate_importance_score(
                user_query, ai_response, model_used, context
            )

            # TTL ê³„ì‚°
            ttl_days = self.IMPORTANCE_LEVELS[importance_score]["ttl_days"]
            expires_at = None
            if ttl_days > 0:
                expires_at = datetime.now() + timedelta(days=ttl_days)

            with self.transaction(project_id) as conn:
                cursor = conn.execute("""
                    INSERT INTO conversations (
                        user_query, ai_response, model_used, importance_score,
                        tags, session_id, token_count, response_time_ms,
                        project_context, expires_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    user_query, ai_response, model_used, importance_score,
                    json.dumps(tags or [], ensure_ascii=False), session_id,
                    token_count, response_time_ms, json.dumps(context, ensure_ascii=False),
                    expires_at
                ))

                conversation_id = cursor.lastrowid

                # ì„ë² ë”© íì— ì¶”ê°€ (ë‚˜ì¤‘ì— ë¹„ë™ê¸° ì²˜ë¦¬)
                conn.execute("""
                    INSERT INTO conversation_embeddings (conversation_id, sync_status)
                    VALUES (?, 'pending')
                """, (conversation_id,))

                return conversation_id

        except (OSError, PermissionError) as e:
            print(f"âš ï¸ Cannot save conversation to memory: {e}")
            return None
        except Exception as e:
            print(f"âš ï¸ Memory save error: {e}")
            return None

    def search_conversations(self, project_id: str, query: str = None,
                           importance_min: int = None, limit: int = 10,
                           offset: int = 0, use_advanced_ranking: bool = True) -> List[Dict[str, Any]]:
        """
        ëŒ€í™” ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì¤‘ìš”ë„ í•„í„° + ê³ ê¸‰ ë­í‚¹)
        """
        if not self._storage_available:
            return []

        try:
            with self.transaction(project_id) as conn:
                if query:
                    if use_advanced_ranking:
                        # ê³ ê¸‰ FTS5 ê²€ìƒ‰ (BM25 + ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜)
                        cursor = conn.execute("""
                            SELECT c.*,
                                   bm25(conversations_fts) as relevance_score,
                                   (bm25(conversations_fts) + (c.importance_score * 0.1)) as combined_score
                            FROM conversations c
                            JOIN conversations_fts fts ON c.id = fts.rowid
                            WHERE conversations_fts MATCH ?
                            AND (? IS NULL OR c.importance_score >= ?)
                            ORDER BY combined_score DESC, c.timestamp DESC
                            LIMIT ? OFFSET ?
                        """, (query, importance_min, importance_min, limit, offset))
                    else:
                        # ê¸°ë³¸ FTS5 ê²€ìƒ‰
                        cursor = conn.execute("""
                            SELECT c.* FROM conversations c
                            JOIN conversations_fts fts ON c.id = fts.rowid
                            WHERE conversations_fts MATCH ?
                            AND (? IS NULL OR c.importance_score >= ?)
                            ORDER BY c.timestamp DESC
                            LIMIT ? OFFSET ?
                        """, (query, importance_min, importance_min, limit, offset))
                else:
                    # ì¼ë°˜ ê²€ìƒ‰ (ì¤‘ìš”ë„ ê¸°ë°˜)
                    cursor = conn.execute("""
                        SELECT * FROM conversations
                        WHERE (? IS NULL OR importance_score >= ?)
                        ORDER BY importance_score DESC, timestamp DESC
                        LIMIT ? OFFSET ?
                    """, (importance_min, importance_min, limit, offset))

                results = []
                for row in cursor.fetchall():
                    result = dict(row)
                    result['tags'] = json.loads(result['tags'] or '[]')
                    result['project_context'] = json.loads(result['project_context'] or '{}')
                    # ê²€ìƒ‰ ì ìˆ˜ ì •ë³´ í¬í•¨
                    if 'relevance_score' in result:
                        result['search_metadata'] = {
                            'relevance_score': result.get('relevance_score', 0),
                            'combined_score': result.get('combined_score', 0),
                            'search_type': 'fts5_advanced'
                        }
                    results.append(result)

                return results

        except (OSError, PermissionError) as e:
            print(f"âš ï¸ Cannot search conversations: {e}")
            return []
        except Exception as e:
            print(f"âš ï¸ Search error: {e}")
            return []

    def get_conversation_stats(self, project_id: str) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬ í†µê³„"""
        if not self._storage_available:
            return {
                'total_conversations': 0,
                'avg_importance': 0,
                'oldest_conversation': None,
                'latest_conversation': None,
                'importance_distribution': {},
                'model_usage': {}
            }

        try:
            with self.transaction(project_id) as conn:
                # ê¸°ë³¸ í†µê³„
                cursor = conn.execute("""
                    SELECT
                        COUNT(*) as total_conversations,
                        AVG(importance_score) as avg_importance,
                        MIN(timestamp) as oldest_conversation,
                        MAX(timestamp) as latest_conversation
                    FROM conversations
                """)
                stats = dict(cursor.fetchone())

                # ì¤‘ìš”ë„ë³„ ë¶„í¬
                cursor = conn.execute("""
                    SELECT importance_score, COUNT(*) as count
                    FROM conversations
                    GROUP BY importance_score
                    ORDER BY importance_score
                """)
                stats['importance_distribution'] = {
                    row['importance_score']: row['count']
                    for row in cursor.fetchall()
                }

                # ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰
                cursor = conn.execute("""
                    SELECT model_used, COUNT(*) as count
                    FROM conversations
                    WHERE model_used IS NOT NULL
                    GROUP BY model_used
                """)
                stats['model_usage'] = {
                    row['model_used']: row['count']
                    for row in cursor.fetchall()
                }

                return stats

        except (OSError, PermissionError) as e:
            print(f"âš ï¸ Cannot get conversation stats: {e}")
            return {
                'total_conversations': 0,
                'avg_importance': 0,
                'oldest_conversation': None,
                'latest_conversation': None,
                'importance_distribution': {},
                'model_usage': {}
            }
        except Exception as e:
            print(f"âš ï¸ Stats error: {e}")
            return {
                'total_conversations': 0,
                'avg_importance': 0,
                'oldest_conversation': None,
                'latest_conversation': None,
                'importance_distribution': {},
                'model_usage': {}
            }

    # ============ ë²¡í„° ì„ë² ë”© ë° ê²€ìƒ‰ ë©”ì„œë“œ ============

    def _get_qdrant_client(self) -> Optional[QdrantClient]:
        """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ë°˜í™˜"""
        if not self._vector_enabled:
            return None

        if self._qdrant_client is None:
            try:
                self._qdrant_client = QdrantClient(url=self.qdrant_url, timeout=30.0)
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                _ = self._qdrant_client.get_collections()
            except Exception as e:
                print(f"âš ï¸ Qdrant connection failed: {e}")
                print("ğŸ’¡ Vector search will be disabled for this session.")
                self._vector_enabled = False
                return None

        return self._qdrant_client

    async def _get_embeddings(self, texts: List[str]) -> Optional[List[List[float]]]:
        """FastEmbed ì„œë¹„ìŠ¤ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if not self._vector_enabled or not texts:
            return None

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.embedding_url}/embed",
                    json={"texts": texts}
                )
                response.raise_for_status()
                data = response.json()

                # ì°¨ì› ì •ë³´ ì €ì¥
                if self._embedding_dim is None:
                    self._embedding_dim = data.get('dim', 384)

                return data['embeddings']

        except Exception as e:
            print(f"âš ï¸ Embedding generation failed: {e}")
            return None


    async def _store_conversation_vectors(self, project_id: str, conversation_id: int,
                                        user_query: str, ai_response: str):
        """ëŒ€í™” ë²¡í„°ë¥¼ Qdrantì— ì €ì¥"""
        if not self._vector_enabled:
            return

        try:
            # ëŒ€í™” í…ìŠ¤íŠ¸ ê²°í•© (ê²€ìƒ‰ìš©)
            combined_text = f"Q: {user_query}\nA: {ai_response}"

            # ì„ë² ë”© ìƒì„±
            embeddings = await self._get_embeddings([combined_text])
            if not embeddings:
                return

            # Qdrantì— ì €ì¥
            qdrant = self._get_qdrant_client()
            if not qdrant:
                return

            collection_name = f"memory_{project_id[:8]}"
            if not self.ensure_memory_collection(project_id):
                return

            # í¬ì¸íŠ¸ ìƒì„±
            point = qmodels.PointStruct(
                id=conversation_id,
                vector=embeddings[0],
                payload={
                    "conversation_id": conversation_id,
                    "project_id": project_id,
                    "user_query": user_query[:500],  # í˜ì´ë¡œë“œ í¬ê¸° ì œí•œ
                    "ai_response": ai_response[:1000],
                    "timestamp": datetime.now().isoformat(),
                    "combined_text": combined_text[:1500]
                }
            )

            qdrant.upsert(collection_name=collection_name, points=[point])

            # SQLiteì— ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
            with self.transaction(project_id) as conn:
                conn.execute("""
                    UPDATE conversation_embeddings
                    SET sync_status = 'synced', synced_at = CURRENT_TIMESTAMP
                    WHERE conversation_id = ?
                """, (conversation_id,))

        except Exception as e:
            print(f"âš ï¸ Vector storage failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                with self.transaction(project_id) as conn:
                    conn.execute("""
                        UPDATE conversation_embeddings
                        SET sync_status = 'failed'
                        WHERE conversation_id = ?
                    """, (conversation_id,))
            except:
                pass  # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ì›ë³¸ ì˜¤ë¥˜ê°€ ì¤‘ìš”

    async def vector_search_conversations(self, project_id: str, query: str,
                                        limit: int = 5, score_threshold: float = 0.7) -> List[Dict]:
        """ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ëŒ€í™” ê²€ìƒ‰"""
        if not self._vector_enabled:
            return []

        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            embeddings = await self._get_embeddings([query])
            if not embeddings:
                return []

            # Qdrantì—ì„œ ê²€ìƒ‰
            qdrant = self._get_qdrant_client()
            if not qdrant:
                return []

            collection_name = f"memory_{project_id[:8]}"

            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            if not self.ensure_memory_collection(project_id):
                return []

            search_result = qdrant.search(
                collection_name=collection_name,
                query_vector=embeddings[0],
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True
            )

            # ê²°ê³¼ ë³€í™˜
            results = []
            for hit in search_result:
                results.append({
                    'conversation_id': hit.payload['conversation_id'],
                    'user_query': hit.payload['user_query'],
                    'ai_response': hit.payload['ai_response'],
                    'similarity_score': hit.score,
                    'timestamp': hit.payload['timestamp']
                })

            return results

        except Exception as e:
            print(f"âš ï¸ Vector search failed: {e}")
            return []

    async def process_pending_embeddings(self, project_id: str, batch_size: int = 10):
        """ëŒ€ê¸° ì¤‘ì¸ ëŒ€í™”ë“¤ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        if not self._vector_enabled:
            return 0

        try:
            with self.transaction(project_id) as conn:
                # ëŒ€ê¸° ì¤‘ì¸ ëŒ€í™”ë“¤ ì¡°íšŒ
                cursor = conn.execute("""
                    SELECT ce.conversation_id, c.user_query, c.ai_response
                    FROM conversation_embeddings ce
                    JOIN conversations c ON ce.conversation_id = c.id
                    WHERE ce.sync_status = 'pending'
                    ORDER BY c.timestamp
                    LIMIT ?
                """, (batch_size,))

                pending_conversations = cursor.fetchall()

            if not pending_conversations:
                return 0

            # ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ê°œë³„ ì‹¤íŒ¨ ì²˜ë¦¬)
            processed = 0
            for conv in pending_conversations:
                try:
                    await self._store_conversation_vectors(
                        project_id=project_id,
                        conversation_id=conv['conversation_id'],
                        user_query=conv['user_query'],
                        ai_response=conv['ai_response']
                    )
                    processed += 1
                except Exception as e:
                    print(f"âš ï¸ Failed to process conversation {conv['conversation_id']}: {e}")
                    # ê°œë³„ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ëŒ€í™”ë§Œ failed ìƒíƒœë¡œ ë§ˆí‚¹
                    try:
                        with self.transaction(project_id) as conn:
                            conn.execute("""
                                UPDATE conversation_embeddings
                                SET sync_status = 'failed'
                                WHERE conversation_id = ?
                            """, (conv['conversation_id'],))
                    except:
                        pass  # ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ëŒ€í™” ê³„ì† ì²˜ë¦¬

            return processed

        except Exception as e:
            print(f"âš ï¸ Embedding batch processing failed: {e}")
            return 0

    async def hybrid_search_conversations(self, project_id: str, query: str,
                                        limit: int = 10, combine_results: bool = True) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FTS5 + ë²¡í„° ìœ ì‚¬ë„)"""
        # FTS5 ê²€ìƒ‰ ê²°ê³¼
        fts_results = self.search_conversations(project_id, query, limit=limit)

        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        vector_results = await self.vector_search_conversations(
            project_id, query, limit=limit, score_threshold=0.6
        )

        if not combine_results:
            return {
                'fts_results': fts_results,
                'vector_results': vector_results
            }

        # ê²°ê³¼ ê²°í•© ë° ì¤‘ë³µ ì œê±°
        combined_results = {}

        # FTS5 ê²°ê³¼ ì¶”ê°€ (ë†’ì€ ê°€ì¤‘ì¹˜)
        for result in fts_results:
            conv_id = result['id']
            combined_results[conv_id] = {
                **result,
                'search_score': 1.0,  # FTS5ëŠ” ê¸°ë³¸ ì ìˆ˜ 1.0
                'search_method': 'fts5'
            }

        # ë²¡í„° ê²°ê³¼ ì¶”ê°€ ë˜ëŠ” ì ìˆ˜ ë³´ê°•
        for result in vector_results:
            conv_id = result['conversation_id']
            if conv_id in combined_results:
                # ì´ë¯¸ ìˆëŠ” ê²½ìš° ì ìˆ˜ ë³´ê°•
                combined_results[conv_id]['search_score'] = max(
                    combined_results[conv_id]['search_score'],
                    result['similarity_score']
                )
                combined_results[conv_id]['search_method'] = 'hybrid'
                combined_results[conv_id]['similarity_score'] = result['similarity_score']
            else:
                # ìƒˆë¡œ ì¶”ê°€
                combined_results[conv_id] = {
                    'id': conv_id,
                    'user_query': result['user_query'],
                    'ai_response': result['ai_response'],
                    'timestamp': result['timestamp'],
                    'search_score': result['similarity_score'],
                    'similarity_score': result['similarity_score'],
                    'search_method': 'vector'
                }

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ì œí•œ
        final_results = sorted(
            combined_results.values(),
            key=lambda x: x['search_score'],
            reverse=True
        )[:limit]

        return final_results

    # ============ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ============

    def rebuild_fts_index(self, project_id: str) -> bool:
        """FTS5 ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        try:
            with self.transaction(project_id) as conn:
                # FTS5 í…Œì´ë¸” ë°ì´í„° ì •ë¦¬ ë° ì¬êµ¬ì¶•
                conn.execute("DELETE FROM conversations_fts")
                conn.execute("""
                    INSERT INTO conversations_fts(rowid, user_query, ai_response)
                    SELECT id, user_query, ai_response FROM conversations
                """)
                return True
        except Exception as e:
            print(f"âš ï¸ FTS index rebuild failed: {e}")
            return False

    def ensure_memory_collection(self, project_id: str) -> bool:
        """Qdrant ë©”ëª¨ë¦¬ ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            collection_name = f"memory_{project_id[:8]}"

            # HTTP APIë¡œ ì»¬ë ‰ì…˜ í™•ì¸
            import requests
            response = requests.get(f"{self.qdrant_url}/collections/{collection_name}")

            if response.status_code == 404:
                # ì»¬ë ‰ì…˜ ìƒì„±
                create_data = {
                    "vectors": {
                        "size": 384,  # BAAI/bge-small-en-v1.5 ì°¨ì›
                        "distance": "Cosine"
                    }
                }

                response = requests.put(
                    f"{self.qdrant_url}/collections/{collection_name}",
                    json=create_data,
                    timeout=30
                )

                if response.status_code == 200:
                    print(f"âœ… Qdrant ì»¬ë ‰ì…˜ ìƒì„±ë¨: {collection_name}")
                    # ì»¬ë ‰ì…˜ ìƒì„± ì„±ê³µ ì‹œ ë²¡í„° ê¸°ëŠ¥ ìë™ í™œì„±í™”
                    if not self._vector_enabled:
                        self._vector_enabled = True
                        print(f"ğŸ”„ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ ìë™ ë³µêµ¬ë¨")
                    return True
                else:
                    print(f"âš ï¸ Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                    return False
            elif response.status_code == 200:
                # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸ ì„±ê³µ ì‹œ ë²¡í„° ê¸°ëŠ¥ ìë™ í™œì„±í™”
                if not self._vector_enabled:
                    self._vector_enabled = True
                    print(f"ğŸ”„ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ ìë™ ë³µêµ¬ë¨")
                return True
            else:
                print(f"âš ï¸ Qdrant ì»¬ë ‰ì…˜ í™•ì¸ ì‹¤íŒ¨: {response.status_code}")
                return False

        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"Qdrant ì»¬ë ‰ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨, ë²¡í„° ê¸°ëŠ¥ ë¹„í™œì„±í™”: {e}")
            self._vector_enabled = False
            print(f"âš ï¸ Qdrant ì»¬ë ‰ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨, FTS ì „ìš© ëª¨ë“œë¡œ ì „í™˜: {e}")
            return False

    def try_vector_recovery(self, project_id: str = None) -> bool:
        """ë²¡í„° ê¸°ëŠ¥ ë³µêµ¬ ì‹œë„"""
        if self._vector_enabled:
            return True  # ì´ë¯¸ í™œì„±í™”ë¨

        if not VECTOR_DEPS_AVAILABLE:
            return False  # ì˜ì¡´ì„± ì—†ìŒ

        try:
            # ê¸°ë³¸ í”„ë¡œì íŠ¸ ID ì‚¬ìš©
            if not project_id:
                project_id = os.getenv('DEFAULT_PROJECT_ID', 'default-project')

            # Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸
            result = self.ensure_memory_collection(project_id)
            if result:
                print(f"âœ… ë²¡í„° ê¸°ëŠ¥ ë³µêµ¬ ì„±ê³µ")
                return True
            else:
                return False

        except Exception as e:
            print(f"âš ï¸ ë²¡í„° ê¸°ëŠ¥ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return False

    def export_memory_backup(self, project_id: str, output_path: Path = None) -> Optional[Path]:
        """ë©”ëª¨ë¦¬ DBë¥¼ JSONìœ¼ë¡œ ë°±ì—…"""
        if not self._storage_available:
            return None

        try:
            if output_path is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_path = self.data_dir / "backups" / f"memory_{project_id}_{timestamp}.json"
                output_path.parent.mkdir(parents=True, exist_ok=True)

            with self.transaction(project_id) as conn:
                conn.row_factory = sqlite3.Row

                export_data = {
                    "export_time": datetime.now().isoformat(),
                    "project_id": project_id,
                    "conversations": [],
                    "embeddings": [],
                    "summaries": [],
                    "important_facts": [],
                    "user_preferences": []
                }

                # ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                tables = [
                    ("conversations", "conversations"),
                    ("conversation_embeddings", "embeddings"),
                    ("conversation_summaries", "summaries"),
                    ("important_facts", "important_facts"),
                    ("user_preferences", "user_preferences")
                ]

                for table_name, export_key in tables:
                    cursor = conn.execute(f"SELECT * FROM {table_name}")
                    for row in cursor.fetchall():
                        export_data[export_key].append(dict(row))

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            return output_path

        except Exception as e:
            print(f"âš ï¸ ë°±ì—… ì‹¤íŒ¨: {e}")
            return None

    def import_memory_backup(self, project_id: str, backup_path: Path) -> bool:
        """JSON ë°±ì—…ì—ì„œ ë©”ëª¨ë¦¬ ë³µì›"""
        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)

            with self.transaction(project_id) as conn:
                # ê¸°ì¡´ ë°ì´í„° ì •ë¦¬ (ì„ íƒì )
                confirmation = input("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ë³µì›í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
                if confirmation.lower() == 'y':
                    conn.execute("DELETE FROM conversation_embeddings")
                    conn.execute("DELETE FROM conversations")
                    conn.execute("DELETE FROM conversation_summaries")
                    conn.execute("DELETE FROM important_facts")
                    conn.execute("DELETE FROM user_preferences")

                # ë°ì´í„° ë³µì›
                for conversation in backup_data.get("conversations", []):
                    conn.execute("""
                        INSERT OR REPLACE INTO conversations (
                            id, timestamp, user_query, ai_response, model_used,
                            importance_score, tags, session_id, token_count,
                            response_time_ms, project_context, created_at,
                            updated_at, expires_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        conversation.get('id'), conversation.get('timestamp'),
                        conversation.get('user_query'), conversation.get('ai_response'),
                        conversation.get('model_used'), conversation.get('importance_score'),
                        conversation.get('tags'), conversation.get('session_id'),
                        conversation.get('token_count'), conversation.get('response_time_ms'),
                        conversation.get('project_context'), conversation.get('created_at'),
                        conversation.get('updated_at'), conversation.get('expires_at')
                    ))

                # ë‹¤ë¥¸ í…Œì´ë¸”ë“¤ë„ ë³µì›...
                # (ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´ conversationsë§Œ ì˜ˆì‹œ)

                # FTS5 ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
                self.rebuild_fts_index(project_id)

            print(f"âœ… ë°±ì—… ë³µì› ì™„ë£Œ: {backup_path}")
            return True

        except Exception as e:
            print(f"âš ï¸ ë°±ì—… ë³µì› ì‹¤íŒ¨: {e}")
            return False

    def cleanup_expired_conversations(self, project_id: str) -> int:
        """TTL ë§Œë£Œëœ ëŒ€í™” ì •ë¦¬"""
        try:
            with self.transaction(project_id) as conn:
                cursor = conn.execute("""
                    DELETE FROM conversations
                    WHERE expires_at IS NOT NULL
                    AND expires_at < ?
                """, (datetime.now().isoformat(),))

                deleted_count = cursor.rowcount

                # ê³ ì•„ ì„ë² ë”© ì •ë¦¬
                conn.execute("""
                    DELETE FROM conversation_embeddings
                    WHERE conversation_id NOT IN (
                        SELECT id FROM conversations
                    )
                """)

                if deleted_count > 0:
                    print(f"âœ… TTL ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ëŒ€í™” ì‚­ì œ")

                return deleted_count

        except Exception as e:
            print(f"âš ï¸ TTL ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    def optimize_database(self, project_id: str) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM, ANALYZE)"""
        try:
            with self.transaction(project_id) as conn:
                conn.execute("VACUUM")
                conn.execute("ANALYZE")
                print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ: {project_id}")
                return True
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            return False

    def get_qdrant_sync_queue(self, project_id: str, limit: int = 100,
                            include_failed: bool = False) -> List[Dict]:
        """Qdrant ë™ê¸°í™” ëŒ€ê¸°ì—´ ì¡°íšŒ"""
        try:
            with self.transaction(project_id) as conn:
                if include_failed:
                    # ì‹¤íŒ¨í•œ ê²ƒë„ í¬í•¨ (ì¬ì‹œë„ìš©)
                    cursor = conn.execute("""
                        SELECT ce.*, c.user_query, c.ai_response, c.model_used, c.importance_score
                        FROM conversation_embeddings ce
                        JOIN conversations c ON ce.conversation_id = c.id
                        WHERE ce.sync_status IN ('pending', 'failed')
                        ORDER BY
                            CASE WHEN ce.sync_status = 'pending' THEN 0 ELSE 1 END,
                            c.importance_score DESC,
                            c.created_at
                        LIMIT ?
                    """, (limit,))
                else:
                    cursor = conn.execute("""
                        SELECT ce.*, c.user_query, c.ai_response, c.model_used, c.importance_score
                        FROM conversation_embeddings ce
                        JOIN conversations c ON ce.conversation_id = c.id
                        WHERE ce.sync_status = 'pending'
                        ORDER BY c.importance_score DESC, c.created_at
                        LIMIT ?
                    """, (limit,))

                return [dict(row) for row in cursor.fetchall()]

        except Exception as e:
            print(f"âš ï¸ ë™ê¸°í™” í ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def batch_sync_to_qdrant(self, project_id: str, batch_size: int = 64) -> Dict[str, int]:
        """Qdrant ë°°ì¹˜ ë™ê¸°í™” (ê°œì„ ëœ ì²˜ë¦¬)"""
        sync_stats = {"synced": 0, "failed": 0, "skipped": 0}

        # ë²¡í„° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ì¦‰ì‹œ ë°˜í™˜
        if not self._vector_enabled:
            return sync_stats

        try:
            # ì»¬ë ‰ì…˜ ë¨¼ì € í™•ì¸/ìƒì„±
            if not self.ensure_memory_collection(project_id):
                return sync_stats

            # ëŒ€ê¸°ì—´ ì¡°íšŒ
            sync_queue = self.get_qdrant_sync_queue(project_id, batch_size, include_failed=True)
            if not sync_queue:
                return sync_stats

            # ì„ë² ë”© ìƒì„±ì´ í•„ìš”í•œ í•­ëª©ë“¤
            texts_to_embed = []
            items_map = {}

            for item in sync_queue:
                conv_id = item['conversation_id']
                combined_text = f"Q: {item['user_query']}\nA: {item['ai_response']}"
                texts_to_embed.append(combined_text)
                items_map[len(texts_to_embed) - 1] = item

            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            import requests
            try:
                response = requests.post(
                    f"{self.embedding_url}/embed",
                    json={"texts": texts_to_embed},
                    timeout=60
                )
                response.raise_for_status()
                embeddings_data = response.json()
                embeddings = embeddings_data['embeddings']
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                # ëª¨ë“  í•­ëª©ì„ failedë¡œ ë§ˆí‚¹
                self._mark_embeddings_failed(project_id, [item['conversation_id'] for item in sync_queue])
                sync_stats["failed"] = len(sync_queue)
                return sync_stats

            # Qdrantì— ë°°ì¹˜ ì—…ë¡œë“œ
            collection_name = f"memory_{project_id[:8]}"
            points_data = []

            for idx, embedding in enumerate(embeddings):
                item = items_map[idx]  # items_mapì€ ë°°ì¹˜ ì¸ë±ìŠ¤ â†’ ëŒ€í™” ì •ë³´ êµ¬ì¡°

                point = {
                    "id": item['conversation_id'],
                    "vector": embedding,
                    "payload": {
                        "conversation_id": item['conversation_id'],
                        "project_id": project_id,
                        "user_query": item['user_query'][:500],
                        "ai_response": item['ai_response'][:1000],
                        "model_used": item.get('model_used', ''),
                        "importance_score": item.get('importance_score', 5),
                        "created_at": item['created_at']
                    }
                }
                points_data.append(point)

            # Qdrant ë°°ì¹˜ ì—…ë¡œë“œ
            try:
                response = requests.put(
                    f"{self.qdrant_url}/collections/{collection_name}/points",
                    json={"points": points_data},
                    timeout=60
                )
                response.raise_for_status()

                # ì„±ê³µí•œ í•­ëª©ë“¤ ìƒíƒœ ì—…ë°ì´íŠ¸
                with self.transaction(project_id) as conn:
                    for idx, item in enumerate(sync_queue):
                        conn.execute("""
                            UPDATE conversation_embeddings
                            SET sync_status = 'synced',
                                synced_at = ?,
                                embedding_vector = ?
                            WHERE conversation_id = ?
                        """, (
                            datetime.now().isoformat(),
                            json.dumps(embeddings[idx]),  # ë°°ì¹˜ ì¸ë±ìŠ¤ ì‚¬ìš©
                            item['conversation_id']
                        ))

                sync_stats["synced"] = len(sync_queue)

            except Exception as e:
                print(f"âš ï¸ Qdrant ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._mark_embeddings_failed(project_id, [item['conversation_id'] for item in sync_queue])
                sync_stats["failed"] = len(sync_queue)

            return sync_stats

        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return sync_stats

    def _mark_embeddings_failed(self, project_id: str, conversation_ids: List[int]):
        """ì„ë² ë”© ë™ê¸°í™” ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí‚¹"""
        try:
            with self.transaction(project_id) as conn:
                placeholders = ','.join(['?' for _ in conversation_ids])
                conn.execute(f"""
                    UPDATE conversation_embeddings
                    SET sync_status = 'failed'
                    WHERE conversation_id IN ({placeholders})
                """, conversation_ids)
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨ ìƒíƒœ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

    def retry_failed_syncs(self, project_id: str, max_retries: int = 3) -> Dict[str, int]:
        """ì‹¤íŒ¨í•œ ë™ê¸°í™” ì¬ì‹œë„"""
        stats = {"retried": 0, "succeeded": 0, "still_failed": 0}

        try:
            # ì‹¤íŒ¨í•œ í•­ëª©ë“¤ ì¤‘ ì¬ì‹œë„ íšŸìˆ˜ê°€ ì ì€ ê²ƒë“¤ ì¡°íšŒ
            with self.transaction(project_id) as conn:
                cursor = conn.execute("""
                    SELECT ce.conversation_id, c.user_query, c.ai_response
                    FROM conversation_embeddings ce
                    JOIN conversations c ON ce.conversation_id = c.id
                    WHERE ce.sync_status = 'failed'
                    LIMIT 50
                """)

                failed_items = cursor.fetchall()

            if not failed_items:
                return stats

            # ê°œë³„ ì¬ì‹œë„ (ë°°ì¹˜ë³´ë‹¤ ì•ˆì „)
            for item in failed_items:
                try:
                    # ì¬ì‹œë„
                    combined_text = f"Q: {item['user_query']}\nA: {item['ai_response']}"

                    # ì„ë² ë”© ìƒì„±
                    response = requests.post(
                        f"{self.embedding_url}/embed",
                        json={"texts": [combined_text]},
                        timeout=30
                    )
                    response.raise_for_status()
                    embedding = response.json()['embeddings'][0]

                    # Qdrant ì—…ë¡œë“œ
                    collection_name = f"memory_{project_id[:8]}"
                    point_data = {
                        "points": [{
                            "id": item['conversation_id'],
                            "vector": embedding,
                            "payload": {
                                "conversation_id": item['conversation_id'],
                                "project_id": project_id,
                                "user_query": item['user_query'][:500],
                                "ai_response": item['ai_response'][:1000],
                                "retry_attempt": True
                            }
                        }]
                    }

                    response = requests.put(
                        f"{self.qdrant_url}/collections/{collection_name}/points",
                        json=point_data,
                        timeout=30
                    )
                    response.raise_for_status()

                    # ì„±ê³µ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
                    with self.transaction(project_id) as conn:
                        conn.execute("""
                            UPDATE conversation_embeddings
                            SET sync_status = 'synced',
                                synced_at = ?,
                                embedding_vector = ?
                            WHERE conversation_id = ?
                        """, (
                            datetime.now().isoformat(),
                            json.dumps(embedding),
                            item['conversation_id']
                        ))

                    stats["succeeded"] += 1

                except Exception as e:
                    print(f"âš ï¸ ì¬ì‹œë„ ì‹¤íŒ¨ - ëŒ€í™” {item['conversation_id']}: {e}")
                    stats["still_failed"] += 1

                stats["retried"] += 1

            return stats

        except Exception as e:
            print(f"âš ï¸ ì¬ì‹œë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return stats

# ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_current_instance = None

def get_memory_system() -> MemorySystem:
    """í˜„ì¬ í™œì„±í™”ëœ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _current_instance
    if _current_instance is None:
        _current_instance = MemorySystem()
    return _current_instance

def set_memory_system(instance: MemorySystem):
    """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë³€ê²½"""
    global _current_instance
    _current_instance = instance

# ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)
memory_system = get_memory_system()