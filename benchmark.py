#!/usr/bin/env python3
"""
MCP Security ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸
"""

import time
import sys
import os
import statistics
from pathlib import Path

# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìƒëŒ€ ì„í¬íŠ¸ ì„¤ì •
sys.path.append(os.path.join(os.path.dirname(__file__), 'services', 'mcp-server'))

from security import SecurityValidator, SecureExecutionEnvironment
from safe_api import SecurePathValidator, SafeFileAPI


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì •ê¸°"""

    def __init__(self):
        self.validator = SecurityValidator("normal")
        self.strict_validator = SecurityValidator("strict")
        self.executor = SecureExecutionEnvironment(self.validator)
        self.path_validator = SecurePathValidator()
        self.file_api = SafeFileAPI()

    def time_function(self, func, *args, **kwargs):
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return end_time - start_time, result

    def benchmark_ast_validation(self, iterations=100):
        """AST ê²€ì¦ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ” AST ë³´ì•ˆ ê²€ì¦ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

        test_codes = [
            "import pathlib; print('safe')",
            "import json; data = {'key': 'value'}",
            "import math; result = math.sqrt(16)",
            "import os; print(os.getcwd())",
            "import sys; print(sys.version)"
        ]

        times = []
        for _ in range(iterations):
            start_time = time.time()
            for code in test_codes:
                self.validator.validate_code(code)
            end_time = time.time()
            times.append(end_time - start_time)

        avg_time = statistics.mean(times)
        total_validations = len(test_codes) * iterations
        validations_per_sec = total_validations / sum(times)

        print(f"  í‰ê·  ì‹œê°„: {avg_time*1000:.2f}ms ({iterations}íšŒ ë°˜ë³µ)")
        print(f"  ì´ˆë‹¹ ê²€ì¦: {validations_per_sec:.1f} validations/sec")
        print(f"  ê²€ì¦ë‹¹ í‰ê· : {(sum(times)/total_validations)*1000:.2f}ms")

        return avg_time

    def benchmark_path_validation(self, iterations=100):
        """ê²½ë¡œ ê²€ì¦ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ›¤ï¸  ê²½ë¡œ ë³´ì•ˆ ê²€ì¦ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

        test_paths = [
            "test.txt",
            "./documents/readme.md",
            "data/config.json",
            "../safe/path.txt",
            "subdir/nested/file.py"
        ]

        times = []
        for _ in range(iterations):
            start_time = time.time()
            for path in test_paths:
                try:
                    self.path_validator.validate_and_resolve_path(path)
                except Exception:
                    # ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ ì„±ëŠ¥ ì¸¡ì •ì—ëŠ” ì˜í–¥ ì—†ìŒ
                    pass
            end_time = time.time()
            times.append(end_time - start_time)

        avg_time = statistics.mean(times)
        total_validations = len(test_paths) * iterations
        validations_per_sec = total_validations / sum(times)

        print(f"  í‰ê·  ì‹œê°„: {avg_time*1000:.2f}ms ({iterations}íšŒ ë°˜ë³µ)")
        print(f"  ì´ˆë‹¹ ê²€ì¦: {validations_per_sec:.1f} validations/sec")
        print(f"  ê²€ì¦ë‹¹ í‰ê· : {(sum(times)/total_validations)*1000:.2f}ms")

        return avg_time

    def benchmark_code_execution(self, iterations=10):
        """ì½”ë“œ ì‹¤í–‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("âš¡ ì½”ë“œ ì‹¤í–‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

        test_codes = [
            "print('Hello World')",
            "import json; print(json.dumps({'key': 'value'}))",
            "import math; print(math.sqrt(16))",
            "import os; print(f'Current dir: {os.getcwd()}')"
        ]

        times = []
        for code in test_codes:
            code_times = []
            for _ in range(iterations):
                start_time = time.time()
                result = self.executor.execute_python_code(code)
                end_time = time.time()
                if result["success"]:
                    code_times.append(end_time - start_time)

            if code_times:
                avg_time = statistics.mean(code_times)
                times.append(avg_time)
                print(f"  ì½”ë“œ: {code[:30]}... => {avg_time*1000:.0f}ms")

        if times:
            overall_avg = statistics.mean(times)
            print(f"  ì „ì²´ í‰ê· : {overall_avg*1000:.0f}ms")
            return overall_avg

        return 0

    def benchmark_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •...")

        try:
            import psutil
            process = psutil.Process()

            # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            # ëŒ€ëŸ‰ ê²€ì¦ ìˆ˜í–‰
            for _ in range(1000):
                self.validator.validate_code("import pathlib; print('test')")

            # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory

            print(f"  ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB")
            print(f"  ìµœì¢… ë©”ëª¨ë¦¬: {final_memory:.1f} MB")
            print(f"  ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f} MB")

            return memory_increase

        except ImportError:
            print("  psutilì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë©”ëª¨ë¦¬ ì¸¡ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return 0

    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ MCP Security ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        print("=" * 50)

        # AST ê²€ì¦ ì„±ëŠ¥
        ast_time = self.benchmark_ast_validation()
        print()

        # ê²½ë¡œ ê²€ì¦ ì„±ëŠ¥
        path_time = self.benchmark_path_validation()
        print()

        # ì½”ë“œ ì‹¤í–‰ ì„±ëŠ¥
        exec_time = self.benchmark_code_execution()
        print()

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_increase = self.benchmark_memory_usage()
        print()

        # ê²°ê³¼ ìš”ì•½
        print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"AST ê²€ì¦ í‰ê·  ì‹œê°„: {ast_time*1000:.2f}ms")
        print(f"ê²½ë¡œ ê²€ì¦ í‰ê·  ì‹œê°„: {path_time*1000:.2f}ms")
        if exec_time > 0:
            print(f"ì½”ë“œ ì‹¤í–‰ í‰ê·  ì‹œê°„: {exec_time*1000:.0f}ms")
        if memory_increase > 0:
            print(f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.1f} MB")

        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        print("\nğŸ¯ ì„±ëŠ¥ í‰ê°€")
        print("=" * 50)

        if ast_time < 0.001:  # 1ms ë¯¸ë§Œ
            print("âœ… AST ê²€ì¦: ìš°ìˆ˜ (< 1ms)")
        elif ast_time < 0.005:  # 5ms ë¯¸ë§Œ
            print("âš ï¸  AST ê²€ì¦: ì–‘í˜¸ (< 5ms)")
        else:
            print("âŒ AST ê²€ì¦: ê°œì„  í•„ìš” (> 5ms)")

        if path_time < 0.001:  # 1ms ë¯¸ë§Œ
            print("âœ… ê²½ë¡œ ê²€ì¦: ìš°ìˆ˜ (< 1ms)")
        elif path_time < 0.005:  # 5ms ë¯¸ë§Œ
            print("âš ï¸  ê²½ë¡œ ê²€ì¦: ì–‘í˜¸ (< 5ms)")
        else:
            print("âŒ ê²½ë¡œ ê²€ì¦: ê°œì„  í•„ìš” (> 5ms)")

        if exec_time > 0:
            if exec_time < 0.1:  # 100ms ë¯¸ë§Œ
                print("âœ… ì½”ë“œ ì‹¤í–‰: ìš°ìˆ˜ (< 100ms)")
            elif exec_time < 0.5:  # 500ms ë¯¸ë§Œ
                print("âš ï¸  ì½”ë“œ ì‹¤í–‰: ì–‘í˜¸ (< 500ms)")
            else:
                print("âŒ ì½”ë“œ ì‹¤í–‰: ê°œì„  í•„ìš” (> 500ms)")

        if memory_increase >= 0:
            if memory_increase < 10:  # 10MB ë¯¸ë§Œ
                print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: ìš°ìˆ˜ (< 10MB ì¦ê°€)")
            elif memory_increase < 50:  # 50MB ë¯¸ë§Œ
                print("âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©: ì–‘í˜¸ (< 50MB ì¦ê°€)")
            else:
                print("âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©: ê°œì„  í•„ìš” (> 50MB ì¦ê°€)")

        print("\nğŸ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


if __name__ == "__main__":
    benchmark = PerformanceBenchmark()
    benchmark.run_all_benchmarks()