services:
  # Chat 모델 서버 (일반 대화용)
  inference-chat:
    image: ghcr.io/ggerganov/llama.cpp:server
    restart: unless-stopped
    ports: ["${INFERENCE_PORT:-8001}:8001"]
    volumes:
      - type: bind
        source: ${MODELS_DIR:-/mnt/e/ai-models}
        target: /models
        read_only: true
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 6G
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: >
      --host 0.0.0.0 --port 8001
      --model /models/${CHAT_MODEL:-Qwen2.5-7B-Instruct-Q4_K_M.gguf}
      -t 4
      -c 1024
      -b 128
      --n-gpu-layers ${LLAMA_N_GPU_LAYERS:-999}
      --parallel ${LLAMA_PARALLEL:-1}
      --cont-batching
      --temp ${LLAMA_TEMP:-0.3}
      --top-p ${LLAMA_TOP_P:-0.9}
      --repeat-penalty ${LLAMA_REPEAT_PENALTY:-1.05}
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8001/health"]
      interval: 20s
      timeout: 5s
      retries: 5

  # Code 모델 서버 (프로그래밍 전용)
  inference-code:
    image: ghcr.io/ggerganov/llama.cpp:server
    restart: unless-stopped
    ports: ["8004:8001"]
    volumes:
      - type: bind
        source: ${MODELS_DIR:-/mnt/e/ai-models}
        target: /models
        read_only: true
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 6G
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: >
      --host 0.0.0.0 --port 8001
      --model /models/${CODE_MODEL:-qwen2.5-coder-7b-instruct-q4_k_m.gguf}
      -t 4
      -c 1024
      -b 128
      --n-gpu-layers ${LLAMA_N_GPU_LAYERS:-999}
      --parallel ${LLAMA_PARALLEL:-1}
      --cont-batching
      --temp ${LLAMA_TEMP:-0.3}
      --top-p ${LLAMA_TOP_P:-0.9}
      --repeat-penalty ${LLAMA_REPEAT_PENALTY:-1.05}
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8001/health"]
      interval: 20s
      timeout: 5s
      retries: 5

  api-gateway:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    ports: ["${API_GATEWAY_PORT:-8000}:8000"]
    volumes:
      - ../services/api-gateway/config.p1.yaml:/app/config.yaml:ro
    platform: linux/amd64
    command: ["--config", "/app/config.yaml", "--port", "8000", "--host", "0.0.0.0"]
    environment:
      - LITELLM_LOG=warning
      - TIMEOUT=60
    depends_on:
      - inference-chat
      - inference-code
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request,sys; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  rag:
    build:
      context: ../services/rag
      dockerfile: Dockerfile
    restart: unless-stopped
    ports: ["${RAG_PORT:-8002}:8002"]
    volumes:
      - ${DATA_DIR:-/mnt/e/ai-data}/documents:/app/documents:ro
      - /:/mnt/host:ro  # Global filesystem access for anywhere document indexing
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBEDDING_URL=${EMBEDDING_URL:-http://embedding:8003}
      # LLM 호출 제한(속도/안정화)
      - RAG_LLM_TIMEOUT=${RAG_LLM_TIMEOUT:-120}
      - RAG_LLM_MAX_TOKENS=${RAG_LLM_MAX_TOKENS:-256}
      - RAG_LLM_TEMPERATURE=${RAG_LLM_TEMPERATURE:-0.3}
      # 인덱싱/검색 튜닝(권장 시작값)
      - RAG_TOPK=${RAG_TOPK:-4}
      - RAG_CHUNK_SIZE=${RAG_CHUNK_SIZE:-512}
      - RAG_CHUNK_OVERLAP=${RAG_CHUNK_OVERLAP:-100}
      - API_GATEWAY_CHAT_MODEL=${API_GATEWAY_CHAT_MODEL:-chat-7b}
      - API_GATEWAY_CODE_MODEL=${API_GATEWAY_CODE_MODEL:-code-7b}
    depends_on:
      - qdrant
      - embedding
      - api-gateway

  embedding:
    build:
      context: ../services/embedding
      dockerfile: Dockerfile
    restart: unless-stopped
    ports: ["${EMBEDDING_PORT:-8003}:8003"]
    environment:
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-BAAI/bge-small-en-v1.5}

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ${DATA_DIR:-/mnt/e/ai-data}/vectors/qdrant:/qdrant/storage

  mcp-server:
    build:
      context: ../services/mcp-server
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "${MCP_PORT:-8020}:8020"
      - "8021:8021"  # Security Admin API port
    volumes:
      # File system access for MCP tools - GLOBAL ACCESS
      - /:/mnt/host:rw  # Full filesystem access for anywhere usage
      - ${DATA_DIR:-/mnt/e/ai-data}:/mnt/data:rw
      - ${MODELS_DIR:-/mnt/e/ai-models}:/mnt/models:ro
      - /mnt/e/local-ai-suite/.git:/mnt/workspace/.git-main:rw  # Main repo .git for worktree
      # Docker socket access for enhanced sandbox functionality
      - /var/run/docker.sock:/var/run/docker.sock:rw
    environment:
      - API_GATEWAY_URL=${API_GATEWAY_URL:-http://api-gateway:8000}
      - RAG_URL=${RAG_URL:-http://rag:8002}
      - EMBEDDING_URL=${EMBEDDING_URL:-http://embedding:8003}
      - NOTION_TOKEN=${NOTION_TOKEN:-}  # Optional Notion integration
      - PROJECT_ROOT=/mnt/workspace  # Align with mount path
      - WORKSPACE_DIR=/mnt/workspace
      - DATA_DIR=/mnt/data
      - GIT_DIR_PATH=/mnt/workspace/.git-main  # Main repo .git for worktree
      - API_GATEWAY_CHAT_MODEL=${API_GATEWAY_CHAT_MODEL:-chat-7b}
      - API_GATEWAY_CODE_MODEL=${API_GATEWAY_CODE_MODEL:-code-7b}
      # Enhanced Security Configuration
      - MCP_SECURITY_LEVEL=${MCP_SECURITY_LEVEL:-normal}
      - USE_ENHANCED_SANDBOX=${USE_ENHANCED_SANDBOX:-true}
      - SANDBOX_MAX_MEMORY_MB=${SANDBOX_MAX_MEMORY_MB:-512}
      - SANDBOX_MAX_CPU_TIME=${SANDBOX_MAX_CPU_TIME:-30}
      - SANDBOX_MAX_OUTPUT_SIZE=${SANDBOX_MAX_OUTPUT_SIZE:-1048576}
      - SANDBOX_MAX_FILE_SIZE=${SANDBOX_MAX_FILE_SIZE:-10485760}
      - SANDBOX_MAX_PROCESSES=${SANDBOX_MAX_PROCESSES:-10}
      - SANDBOX_NETWORK_ACCESS=${SANDBOX_NETWORK_ACCESS:-false}
      - SANDBOX_SESSION_TIMEOUT=${SANDBOX_SESSION_TIMEOUT:-3600}
      - SANDBOX_MAX_SESSIONS=${SANDBOX_MAX_SESSIONS:-50}
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    depends_on:
      - api-gateway
      - rag
      - embedding
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8020/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Memory Maintainer Service - TTL 정리, 백업, Qdrant 동기화
  memory-maintainer:
    image: python:3.11-slim
    restart: unless-stopped
    volumes:
      - ../scripts:/app/scripts:ro  # Memory maintainer script
      - ${DATA_DIR:-/mnt/e/ai-data}/memory:/app/memory:rw  # Memory databases
      - ${DATA_DIR:-/mnt/e/ai-data}/memory/logs:/app/logs:rw  # Logs directory
      - /:/mnt/host:ro  # Global filesystem access for backup operations
    working_dir: /app
    environment:
      - AI_MEMORY_DIR=/app/memory
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBEDDING_URL=${EMBEDDING_URL:-http://embedding:8003}
      - MEMORY_BACKUP_CRON=${MEMORY_BACKUP_CRON:-03:00}
      - MEMORY_SYNC_INTERVAL=${MEMORY_SYNC_INTERVAL:-300}  # 5분
      - TTL_CHECK_INTERVAL=${TTL_CHECK_INTERVAL:-3600}     # 1시간
    depends_on:
      - qdrant
      - embedding
    command: >
      bash -c "
        pip install --no-cache-dir schedule requests &&
        python scripts/memory_maintainer.py
      "
    healthcheck:
      test: ["CMD", "python", "-c", "import os; exit(0 if os.path.exists('/app/logs/memory_maintainer.log') else 1)"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Memory API Service - REST API for memory operations
  memory-api:
    build:
      context: ../services/api-gateway
      dockerfile: Dockerfile.memory
    restart: unless-stopped
    ports: ["8005:8005"]
    volumes:
      - ../scripts:/app/scripts:ro  # Memory system script access
      - ${DATA_DIR:-/mnt/e/ai-data}/memory:/app/memory:rw  # Memory databases
      - /:/mnt/host:ro  # Global filesystem access for project path resolution
    environment:
      - AI_MEMORY_DIR=/app/memory
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - EMBEDDING_URL=${EMBEDDING_URL:-http://embedding:8003}
      - DEFAULT_PROJECT_ID=${DEFAULT_PROJECT_ID:-default-project}
    depends_on:
      - qdrant
      - embedding
      - memory-maintainer
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8005/v1/memory/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# No named volumes needed - using external SSD host paths
# SQLite databases are stored as files in ${DATA_DIR}/sqlite/
