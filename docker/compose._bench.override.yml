services:
  inference:
    command: [
      "--host","0.0.0.0",
      "--port","8001",
      "--model","/models/qwen2.5-14b-instruct-q4_k_m.gguf",
      "--parallel","2",
      "--ctx-size","4096",
      "--n-gpu-layers","24",
      "--timeout","600"
    ]
