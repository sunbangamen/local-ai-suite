# LiteLLM OpenAI 호환 게이트웨이 설정
# 클라이언트에서는 model: "local-7b" 로 호출
model_list:
  - model_name: local-7b
    litellm_params:
      model: openai/chat/completions
      api_base: http://inference:8001/v1
      api_key: sk-noop
      timeout: 60
      stream: true
      max_output_tokens: 256